{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from __future__ import division\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/creditcard.csv')\n",
    "df_fe = df.drop(['V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_normal_transacation = len(df_fe[df_fe[\"Class\"]==0]) # normal transaction are repersented by 0\n",
    "count_fraud_transacation = len(df_fe[df_fe[\"Class\"]==1]) # fraud by 1\n",
    "fraud_indices = np.array(df_fe[df_fe.Class==1].index)\n",
    "normal_indices = np.array(df_fe[df_fe.Class==0].index)\n",
    "\n",
    "#now let us a define a function for make undersample data with different proportion\n",
    "#different proportion means with different proportion of normal classes of data\n",
    "\n",
    "def undersample(df, normal_indices, fraud_indices, multiple): # multiple denote the normal data = multiple * fraud data\n",
    "    normal_indices_undersample = np.array(np.random.choice(normal_indices,(multiple*count_fraud_transacation),replace=False))\n",
    "    undersample_data = np.concatenate([fraud_indices, normal_indices_undersample])\n",
    "    undersample_data = df.iloc[undersample_data,:]\n",
    "    \n",
    "    print \"the normal transacation proportion is :\", len(undersample_data[undersample_data.Class==0])/len(undersample_data)\n",
    "    print \"the fraud transacation proportion is :\", len(undersample_data[undersample_data.Class==1])/len(undersample_data)\n",
    "    print \"total number of record in resampled data is:\",len(undersample_data)\n",
    "    return(undersample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_algorithms(model, features_train, features_test, labels_train, labels_test):\n",
    "    model.fit(features_train, labels_train.values.ravel())\n",
    "    pred = model.predict(features_test)\n",
    "    cm = confusion_matrix(labels_test,pred)\n",
    "    recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    print \"the recall for this model is :\", recall\n",
    "    print \"The accuracy is :\", (cm[1,1]+cm[0,0])/(cm[0,0] + cm[0,1] + cm[1,0] + cm[1,1])\n",
    "    loss = (1 - precision) * 88.29 + (1 - recall) * 122.12\n",
    "    fig= plt.figure(figsize=(6,3))# to plot the graph\n",
    "    print \"TP\",cm[1,1] # no of fraud transaction which are predicted fraud\n",
    "    print \"TN\",cm[0,0] # no. of normal transaction which are predited normal\n",
    "    print \"FP\",cm[0,1] # no of normal transaction which are predicted fraud\n",
    "    print \"FN\",cm[1,0] # no of fraud Transaction which are predicted normal\n",
    "    sns.heatmap(cm, cmap=\"coolwarm_r\", annot=True, linewidths=0.5)\n",
    "    plt.title(\"Confusion_matrix\")\n",
    "    plt.xlabel(\"Predicted_class\")\n",
    "    plt.ylabel(\"Real class\")\n",
    "    plt.show()\n",
    "    print \"Classification Report:\" \n",
    "    print(classification_report(labels_test,pred))\n",
    "    print \"The loss is : \", loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_fuction(model,features_train,features_test,labels_train,labels_test):\n",
    "    model.fit(features_train,labels_train.values.ravel())\n",
    "    pred = model.predict(features_test)\n",
    "    cm = confusion_matrix(labels_test,pred)\n",
    "    recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    loss = (1 - precision) * 88.29 + (1 - recall) * 122.12\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters using custom grid-search like methods with 5-fold cv\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "#### C, penalty\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.992481203008\n",
      "the fraud transacation proportion is : 0.00751879699248\n",
      "total number of record in resampled data is: 65436\n"
     ]
    }
   ],
   "source": [
    "undersample_data_lr = undersample(df, normal_indices,fraud_indices, 132)\n",
    "X_undersample_lr = undersample_data_lr.iloc[:, undersample_data_lr.columns != \"Class\"]\n",
    "y_undersample_lr = undersample_data_lr.iloc[:, undersample_data_lr.columns == \"Class\"]\n",
    "X_undersample_train_lr, X_undersample_test_lr, y_undersample_train_lr, y_undersample_test_lr = train_test_split(X_undersample_lr, y_undersample_lr, random_state=0)\n",
    "X_test_lr = df.iloc[:, df.columns != \"Class\"]\n",
    "y_test_lr = df.iloc[:, df.columns == \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_lr(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    c_param_range = [0.01, 0.1, 1, 10, 100]\n",
    "    penalties = ['l1', 'l2']\n",
    "\n",
    "    penalty_list = []\n",
    "    c_list = []\n",
    "    mean_loss_list = []\n",
    "    \n",
    "    j = 0\n",
    "    for penalty in penalties:\n",
    "        print '-------------------------------------------'\n",
    "        print 'Penalty: ', penalty \n",
    "        print '-------------------------------------------'\n",
    "        for c_param in c_param_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'C parameter: ', c_param\n",
    "            print '-------------------------------------------'\n",
    "            print ''\n",
    "\n",
    "            loss_list = []\n",
    "            for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                # Call the logistic regression model with a certain C parameter\n",
    "                lr = LogisticRegression(C=c_param, penalty=penalty, random_state=0)\n",
    "\n",
    "                # Calculate the custom loss and append it to a list for loss representing the current c_parameter\n",
    "                loss = custom_loss_fuction(lr, X_train_data.iloc[train], X_test_lr, y_train_data.iloc[train], y_test_lr)\n",
    "                loss_list.append(loss)\n",
    "                print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "            j += 1\n",
    "            print ''\n",
    "            print 'Mean loss', np.mean(loss_list)\n",
    "            print ''\n",
    "            penalty_list.append(penalty)\n",
    "            c_list.append(c_param)\n",
    "            mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Penalty', 'C_parameter','Mean loss'])\n",
    "    results_table['Penalty'] = penalty_list\n",
    "    results_table['C_parameter'] = c_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_penalty = results_table.loc[results_table['Mean loss'].idxmin()]['Penalty']\n",
    "    best_c = results_table.loc[results_table['Mean loss'].idxmin()]['C_parameter']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '************************************************************************************'\n",
    "    print 'Best model to choose from cross validation is with Penalty = ', best_penalty, 'and best c = ', best_c\n",
    "    print '************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Penalty:  l1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  53.9483650214455\n",
      "Fold  2 : loss =  52.65091625389262\n",
      "Fold  3 : loss =  57.74639690574804\n",
      "Fold  4 : loss =  48.910385234659195\n",
      "Fold  5 : loss =  54.14612608010897\n",
      "\n",
      "Mean loss 53.480437899170866\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  47.04657095811464\n",
      "Fold  2 : loss =  44.0415245164874\n",
      "Fold  3 : loss =  49.02915595294782\n",
      "Fold  4 : loss =  42.90277155937892\n",
      "Fold  5 : loss =  45.891759863072316\n",
      "\n",
      "Mean loss 45.78235657000022\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  45.60606135136982\n",
      "Fold  2 : loss =  44.19271843371496\n",
      "Fold  3 : loss =  47.17284500840745\n",
      "Fold  4 : loss =  43.7158150886627\n",
      "Fold  5 : loss =  44.90353734282685\n",
      "\n",
      "Mean loss 45.11819544499635\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  45.17236677597868\n",
      "Fold  2 : loss =  43.907075820045364\n",
      "Fold  3 : loss =  47.47180933146911\n",
      "Fold  4 : loss =  43.7158150886627\n",
      "Fold  5 : loss =  44.47707317073171\n",
      "\n",
      "Mean loss 44.94882803737751\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  45.32052001840773\n",
      "Fold  2 : loss =  43.907075820045364\n",
      "Fold  3 : loss =  47.47180933146911\n",
      "Fold  4 : loss =  43.7158150886627\n",
      "Fold  5 : loss =  44.32652245049422\n",
      "\n",
      "Mean loss 44.948348541815825\n",
      "\n",
      "-------------------------------------------\n",
      "Penalty:  l2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  59.25978658536586\n",
      "Fold  2 : loss =  61.92980441156871\n",
      "Fold  3 : loss =  67.33126043542316\n",
      "Fold  4 : loss =  73.400623561896\n",
      "Fold  5 : loss =  60.326203390876415\n",
      "\n",
      "Mean loss 64.44953567702603\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  58.07767605967322\n",
      "Fold  2 : loss =  58.941298807231504\n",
      "Fold  3 : loss =  63.16972360663932\n",
      "Fold  4 : loss =  51.14762697274031\n",
      "Fold  5 : loss =  59.56139819530064\n",
      "\n",
      "Mean loss 58.179544728317\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.9590067568622\n",
      "Fold  2 : loss =  60.46197788617887\n",
      "Fold  3 : loss =  60.43664132615352\n",
      "Fold  4 : loss =  72.9256167835896\n",
      "Fold  5 : loss =  60.18790086641742\n",
      "\n",
      "Mean loss 62.39422872384032\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.9590067568622\n",
      "Fold  2 : loss =  60.63545999144203\n",
      "Fold  3 : loss =  60.13534552845529\n",
      "Fold  4 : loss =  72.9256167835896\n",
      "Fold  5 : loss =  52.87058704453442\n",
      "\n",
      "Mean loss 60.9052032209767\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.9590067568622\n",
      "Fold  2 : loss =  60.63545999144203\n",
      "Fold  3 : loss =  60.13534552845529\n",
      "Fold  4 : loss =  72.9256167835896\n",
      "Fold  5 : loss =  53.00381818181818\n",
      "\n",
      "Mean loss 60.931849448433454\n",
      "\n",
      "  Penalty  C_parameter  Mean loss\n",
      "0      l1         0.01  53.480438\n",
      "1      l1         0.10  45.782357\n",
      "2      l1         1.00  45.118195\n",
      "3      l1        10.00  44.948828\n",
      "4      l1       100.00  44.948349\n",
      "5      l2         0.01  64.449536\n",
      "6      l2         0.10  58.179545\n",
      "7      l2         1.00  62.394229\n",
      "8      l2        10.00  60.905203\n",
      "9      l2       100.00  60.931849\n",
      "\n",
      "************************************************************************************\n",
      "Best model to choose from cross validation is with Penalty =  l1 and best c =  100.0\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_lr = Kfold_tuning_lr(X_undersample_train_lr, y_undersample_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Logistic Regression Model\n",
    "Since C=10 and C=100 have very similar results, we will simply use C=10.\n",
    "* C=10, penalty=l1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 132 proportion\n",
      "the recall for this model is : 0.806910569105691\n",
      "The accuracy is : 0.9993012812185094\n",
      "TP 397\n",
      "TN 284211\n",
      "FP 104\n",
      "FN 95\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.79      0.81      0.80       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n",
      "The loss is :  41.9077459714717\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 132 proportion\"\n",
    "optimal_lr = LogisticRegression(C=10, penalty='l1', random_state=0)\n",
    "prediction_algorithms(optimal_lr, X_undersample_train_lr, X_test_lr, y_undersample_train_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (kernel='rbf')\n",
    "#### C, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.888888888889\n",
      "the fraud transacation proportion is : 0.111111111111\n",
      "total number of record in resampled data is: 4428\n"
     ]
    }
   ],
   "source": [
    "undersample_data_svm = undersample(df_fe, normal_indices,fraud_indices, 8)\n",
    "X_undersample_svm = undersample_data_svm.iloc[:, undersample_data_svm.columns != \"Class\"]\n",
    "y_undersample_svm = undersample_data_svm.iloc[:, undersample_data_svm.columns == \"Class\"]\n",
    "X_undersample_train_svm, X_undersample_test_svm, y_undersample_train_svm, y_undersample_test_svm = train_test_split(X_undersample_svm, y_undersample_svm, random_state=0)\n",
    "X_test_svm = df_fe.iloc[:, df_fe.columns != \"Class\"]\n",
    "y_test_svm = df_fe.iloc[:, df_fe.columns == \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_svm(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    c_param_range = [0.1, 1, 2, 5]\n",
    "    gamma_range = [0.01, 0.1, 'auto', 1]\n",
    "\n",
    "    c_list = []\n",
    "    gamma_list = []\n",
    "    mean_loss_list = []\n",
    "    \n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'C parameter: ', c_param\n",
    "        print '-------------------------------------------'\n",
    "        for gamma in gamma_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'Gamma: ', gamma\n",
    "            print '-------------------------------------------'\n",
    "            print ''\n",
    "\n",
    "            loss_list = []\n",
    "            for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                # Call the logistic regression model with a certain C parameter\n",
    "                svm = SVC(C=c_param, gamma=gamma, random_state=0)\n",
    "\n",
    "                # Calculate the custom loss and append it to a list for loss representing the current c_parameter\n",
    "                loss = custom_loss_fuction(svm, X_train_data.iloc[train], X_test_svm, y_train_data.iloc[train], y_test_svm)\n",
    "                loss_list.append(loss)\n",
    "                print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "            j += 1\n",
    "            print ''\n",
    "            print 'Mean loss', np.mean(loss_list)\n",
    "            print ''\n",
    "            gamma_list.append(gamma)\n",
    "            c_list.append(c_param)\n",
    "            mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['C_parameter','Gamma', 'Mean loss'])\n",
    "    results_table['Gamma'] = gamma_list\n",
    "    results_table['C_parameter'] = c_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_gamma = results_table.loc[results_table['Mean loss'].idxmin()]['Gamma']\n",
    "    best_c = results_table.loc[results_table['Mean loss'].idxmin()]['C_parameter']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '************************************************************************************'\n",
    "    print 'Best model to choose from cross validation is with C = ', best_c, 'and best gamma = ', best_gamma\n",
    "    print '************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcus/venv0/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  49.74240771259065\n",
      "Fold  2 : loss =  46.02070157666433\n",
      "Fold  3 : loss =  45.29219205399602\n",
      "Fold  4 : loss =  45.29219205399602\n",
      "Fold  5 : loss =  44.46191173054588\n",
      "\n",
      "Mean loss 46.16188102555858\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  48.897642276422765\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.18162601626017\n",
      "Fold  4 : loss =  44.18162601626017\n",
      "Fold  5 : loss =  43.93341463414634\n",
      "\n",
      "Mean loss 45.27375609756098\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  48.897642276422765\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.18162601626017\n",
      "Fold  4 : loss =  44.18162601626017\n",
      "Fold  5 : loss =  43.93341463414634\n",
      "\n",
      "Mean loss 45.27375609756098\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  48.897642276422765\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.18162601626017\n",
      "Fold  4 : loss =  44.18162601626017\n",
      "Fold  5 : loss =  43.93341463414634\n",
      "\n",
      "Mean loss 45.27375609756098\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  76.95971609533864\n",
      "Fold  2 : loss =  69.77047521158583\n",
      "Fold  3 : loss =  74.09213271261343\n",
      "Fold  4 : loss =  73.67320410783056\n",
      "Fold  5 : loss =  71.84666876927389\n",
      "\n",
      "Mean loss 73.26843937932847\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  49.19591930344979\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.46191173054588\n",
      "Fold  4 : loss =  44.46191173054588\n",
      "Fold  5 : loss =  44.21281336832355\n",
      "\n",
      "Mean loss 45.501405535516106\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  49.49218773096822\n",
      "Fold  2 : loss =  45.4583622199566\n",
      "Fold  3 : loss =  44.7404234846146\n",
      "Fold  4 : loss =  44.7404234846146\n",
      "Fold  5 : loss =  44.21281336832355\n",
      "\n",
      "Mean loss 45.72884205769552\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  48.897642276422765\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.18162601626017\n",
      "Fold  4 : loss =  44.18162601626017\n",
      "Fold  5 : loss =  43.93341463414634\n",
      "\n",
      "Mean loss 45.27375609756098\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  76.95971609533864\n",
      "Fold  2 : loss =  69.77047521158583\n",
      "Fold  3 : loss =  74.09213271261343\n",
      "Fold  4 : loss =  73.67320410783056\n",
      "Fold  5 : loss =  71.84666876927389\n",
      "\n",
      "Mean loss 73.26843937932847\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  49.19591930344979\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.46191173054588\n",
      "Fold  4 : loss =  44.46191173054588\n",
      "Fold  5 : loss =  44.21281336832355\n",
      "\n",
      "Mean loss 45.501405535516106\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  49.49218773096822\n",
      "Fold  2 : loss =  45.4583622199566\n",
      "Fold  3 : loss =  44.7404234846146\n",
      "Fold  4 : loss =  44.7404234846146\n",
      "Fold  5 : loss =  44.21281336832355\n",
      "\n",
      "Mean loss 45.72884205769552\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  48.897642276422765\n",
      "Fold  2 : loss =  45.174471544715445\n",
      "Fold  3 : loss =  44.18162601626017\n",
      "Fold  4 : loss =  44.18162601626017\n",
      "Fold  5 : loss =  43.93341463414634\n",
      "\n",
      "Mean loss 45.27375609756098\n",
      "\n",
      "    C_parameter Gamma  Mean loss\n",
      "0           0.1  0.01        NaN\n",
      "1           0.1   0.1        NaN\n",
      "2           0.1  auto        NaN\n",
      "3           0.1     1        NaN\n",
      "4           1.0  0.01  46.161881\n",
      "5           1.0   0.1  45.273756\n",
      "6           1.0  auto  45.273756\n",
      "7           1.0     1  45.273756\n",
      "8           2.0  0.01  73.268439\n",
      "9           2.0   0.1  45.501406\n",
      "10          2.0  auto  45.728842\n",
      "11          2.0     1  45.273756\n",
      "12          5.0  0.01  73.268439\n",
      "13          5.0   0.1  45.501406\n",
      "14          5.0  auto  45.728842\n",
      "15          5.0     1  45.273756\n",
      "\n",
      "************************************************************************************\n",
      "Best model to choose from cross validation is with C =  1.0 and best gamma =  0.1\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_svm = Kfold_tuning_svm(X_undersample_train_svm, y_undersample_train_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal SVM Model\n",
    "* C = 1, gamma = 0.1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 8 proportion\n",
      "the recall for this model is : 0.7764227642276422\n",
      "The accuracy is : 0.9996137735378695\n",
      "TP 382\n",
      "TN 284315\n",
      "FP 0\n",
      "FN 110\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       1.00      0.78      0.87       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n",
      "The loss is :  27.303252032520334\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 8 proportion\"\n",
    "optimal_svm = SVC(gamma=0.1, random_state=0)\n",
    "prediction_algorithms(optimal_svm, X_undersample_train_svm, X_test_svm, y_undersample_train_svm, y_test_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### n_estimators, max_features, max_depth\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.985915492958\n",
      "the fraud transacation proportion is : 0.0140845070423\n",
      "total number of record in resampled data is: 34932\n"
     ]
    }
   ],
   "source": [
    "undersample_data_rf = undersample(df_fe, normal_indices,fraud_indices, 70)\n",
    "X_undersample_rf = undersample_data_rf.iloc[:, undersample_data_rf.columns != \"Class\"]\n",
    "y_undersample_rf = undersample_data_rf.iloc[:, undersample_data_rf.columns == \"Class\"]\n",
    "X_undersample_train_rf, X_undersample_test_rf, y_undersample_train_rf, y_undersample_test_rf = train_test_split(X_undersample_rf, y_undersample_rf, random_state=0)\n",
    "X_test_rf = df_fe.iloc[:, df_fe.columns != \"Class\"]\n",
    "y_test_rf = df_fe.iloc[:, df_fe.columns == \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_rf(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    n_estimators_range = [10, 100, 150, 200]\n",
    "    max_features_type = ['auto', 'log2']\n",
    "    max_depth_range = [10, 20, 30, None]\n",
    "    n_estimators_list = []\n",
    "    max_features_list = []\n",
    "    max_depth_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'n_estimators: ', n_estimators\n",
    "        print '-------------------------------------------'\n",
    "        for max_features in max_features_type:\n",
    "            print '-------------------------------------------'\n",
    "            print 'max_features: ', max_features\n",
    "            print '-------------------------------------------'\n",
    "            for max_depth in max_depth_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'max_depth: ', max_depth\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    rf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(rf, X_train_data.iloc[train], X_test_rf, y_train_data.iloc[train], y_test_rf)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                n_estimators_list.append(n_estimators)\n",
    "                max_features_list.append(max_features)\n",
    "                max_depth_list.append(max_depth)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['n_estimators', 'max_features', 'max_depth', 'Mean loss'])\n",
    "    results_table['n_estimators'] = n_estimators_list\n",
    "    results_table['max_features'] = max_features_list\n",
    "    results_table['max_depth'] = max_depth_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['n_estimators']\n",
    "    best_max_features = results_table.loc[results_table['Mean loss'].idxmin()]['max_features']\n",
    "    best_max_depth = results_table.loc[results_table['Mean loss'].idxmin()]['max_depth']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with n_estimators = \", best_n_estimators, \", best max_features = \", best_max_features\n",
    "    print \"and best max_depth = \", best_max_depth\n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "n_estimators:  10\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85760336591693\n",
      "Fold  2 : loss =  34.436267868094546\n",
      "Fold  3 : loss =  35.41204382082431\n",
      "Fold  4 : loss =  34.85525294459907\n",
      "Fold  5 : loss =  36.11754879630509\n",
      "\n",
      "Mean loss 35.13574335914799\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.09404973878636\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  31.03560449634071\n",
      "Fold  4 : loss =  27.198434959349598\n",
      "Fold  5 : loss =  29.006408624955817\n",
      "\n",
      "Mean loss 28.27286813362806\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.515741466488393\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  30.903837398373977\n",
      "Fold  4 : loss =  26.51845832383558\n",
      "Fold  5 : loss =  28.590428297628982\n",
      "\n",
      "Mean loss 28.11166166700695\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.515741466488393\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  30.903837398373977\n",
      "Fold  4 : loss =  26.51845832383558\n",
      "Fold  5 : loss =  28.590428297628982\n",
      "\n",
      "Mean loss 28.11166166700695\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85760336591693\n",
      "Fold  2 : loss =  34.436267868094546\n",
      "Fold  3 : loss =  35.41204382082431\n",
      "Fold  4 : loss =  34.85525294459907\n",
      "Fold  5 : loss =  36.11754879630509\n",
      "\n",
      "Mean loss 35.13574335914799\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.09404973878636\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  31.03560449634071\n",
      "Fold  4 : loss =  27.198434959349598\n",
      "Fold  5 : loss =  29.006408624955817\n",
      "\n",
      "Mean loss 28.27286813362806\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.515741466488393\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  30.903837398373977\n",
      "Fold  4 : loss =  26.51845832383558\n",
      "Fold  5 : loss =  28.590428297628982\n",
      "\n",
      "Mean loss 28.11166166700695\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.515741466488393\n",
      "Fold  2 : loss =  28.029842848707823\n",
      "Fold  3 : loss =  30.903837398373977\n",
      "Fold  4 : loss =  26.51845832383558\n",
      "Fold  5 : loss =  28.590428297628982\n",
      "\n",
      "Mean loss 28.11166166700695\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  100\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85760336591693\n",
      "Fold  2 : loss =  33.733917419596466\n",
      "Fold  3 : loss =  35.83069921838216\n",
      "Fold  4 : loss =  34.99338842326647\n",
      "Fold  5 : loss =  35.515110402808574\n",
      "\n",
      "Mean loss 34.986143765994115\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.870051856734783\n",
      "Fold  2 : loss =  24.75511720267645\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.750994849984092\n",
      "Fold  5 : loss =  24.627943694862477\n",
      "\n",
      "Mean loss 24.642704748580634\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.001093466690065\n",
      "Fold  2 : loss =  24.372238954951822\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.88019047206458\n",
      "Fold  5 : loss =  24.500318416150407\n",
      "\n",
      "Mean loss 24.59265148970045\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.001093466690065\n",
      "Fold  2 : loss =  24.372238954951822\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.88019047206458\n",
      "Fold  5 : loss =  24.500318416150407\n",
      "\n",
      "Mean loss 24.59265148970045\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85760336591693\n",
      "Fold  2 : loss =  33.733917419596466\n",
      "Fold  3 : loss =  35.83069921838216\n",
      "Fold  4 : loss =  34.99338842326647\n",
      "Fold  5 : loss =  35.515110402808574\n",
      "\n",
      "Mean loss 34.986143765994115\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.870051856734783\n",
      "Fold  2 : loss =  24.75511720267645\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.750994849984092\n",
      "Fold  5 : loss =  24.627943694862477\n",
      "\n",
      "Mean loss 24.642704748580634\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.001093466690065\n",
      "Fold  2 : loss =  24.372238954951822\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.88019047206458\n",
      "Fold  5 : loss =  24.500318416150407\n",
      "\n",
      "Mean loss 24.59265148970045\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.001093466690065\n",
      "Fold  2 : loss =  24.372238954951822\n",
      "Fold  3 : loss =  27.209416138645373\n",
      "Fold  4 : loss =  22.88019047206458\n",
      "Fold  5 : loss =  24.500318416150407\n",
      "\n",
      "Mean loss 24.59265148970045\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  150\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.97978192252511\n",
      "Fold  2 : loss =  33.591012607066794\n",
      "Fold  3 : loss =  35.26806550969356\n",
      "Fold  4 : loss =  34.436267868094546\n",
      "Fold  5 : loss =  35.39442561906536\n",
      "\n",
      "Mean loss 34.93391070528908\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.594087120547464\n",
      "Fold  2 : loss =  25.00811846689895\n",
      "Fold  3 : loss =  26.82833006093925\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  24.881841334137718\n",
      "\n",
      "Mean loss 24.45337147981256\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.72494329377764\n",
      "Fold  2 : loss =  24.627943694862477\n",
      "Fold  3 : loss =  26.700390768423816\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  25.00811846689895\n",
      "\n",
      "Mean loss 24.40317532810046\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.72494329377764\n",
      "Fold  2 : loss =  24.627943694862477\n",
      "Fold  3 : loss =  26.700390768423816\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  25.00811846689895\n",
      "\n",
      "Mean loss 24.40317532810046\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.97978192252511\n",
      "Fold  2 : loss =  33.591012607066794\n",
      "Fold  3 : loss =  35.26806550969356\n",
      "Fold  4 : loss =  34.436267868094546\n",
      "Fold  5 : loss =  35.39442561906536\n",
      "\n",
      "Mean loss 34.93391070528908\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.594087120547464\n",
      "Fold  2 : loss =  25.00811846689895\n",
      "Fold  3 : loss =  26.82833006093925\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  24.881841334137718\n",
      "\n",
      "Mean loss 24.45337147981256\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.72494329377764\n",
      "Fold  2 : loss =  24.627943694862477\n",
      "Fold  3 : loss =  26.700390768423816\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  25.00811846689895\n",
      "\n",
      "Mean loss 24.40317532810046\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  23.72494329377764\n",
      "Fold  2 : loss =  24.627943694862477\n",
      "Fold  3 : loss =  26.700390768423816\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  25.00811846689895\n",
      "\n",
      "Mean loss 24.40317532810046\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  200\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85525294459907\n",
      "Fold  2 : loss =  33.86368826945413\n",
      "Fold  3 : loss =  34.99338842326647\n",
      "Fold  4 : loss =  35.130991587950554\n",
      "Fold  5 : loss =  34.561018122013294\n",
      "\n",
      "Mean loss 34.6808678694567\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.448893259900345\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.443131543545647\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.173480697029717\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.70883565621371\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.165270832421072\n",
      "Fold  4 : loss =  22.085347270615557\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.196070404882704\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.70883565621371\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.165270832421072\n",
      "Fold  4 : loss =  22.085347270615557\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.196070404882704\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.85525294459907\n",
      "Fold  2 : loss =  33.86368826945413\n",
      "Fold  3 : loss =  34.99338842326647\n",
      "Fold  4 : loss =  35.130991587950554\n",
      "Fold  5 : loss =  34.561018122013294\n",
      "\n",
      "Mean loss 34.6808678694567\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.448893259900345\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.443131543545647\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.173480697029717\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.70883565621371\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.165270832421072\n",
      "Fold  4 : loss =  22.085347270615557\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.196070404882704\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.70883565621371\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.165270832421072\n",
      "Fold  4 : loss =  22.085347270615557\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.196070404882704\n",
      "\n",
      "    n_estimators max_features  max_depth  Mean loss\n",
      "0             10         auto       10.0  35.135743\n",
      "1             10         auto       20.0  28.272868\n",
      "2             10         auto       30.0  28.111662\n",
      "3             10         auto        NaN  28.111662\n",
      "4             10         log2       10.0  35.135743\n",
      "5             10         log2       20.0  28.272868\n",
      "6             10         log2       30.0  28.111662\n",
      "7             10         log2        NaN  28.111662\n",
      "8            100         auto       10.0  34.986144\n",
      "9            100         auto       20.0  24.642705\n",
      "10           100         auto       30.0  24.592651\n",
      "11           100         auto        NaN  24.592651\n",
      "12           100         log2       10.0  34.986144\n",
      "13           100         log2       20.0  24.642705\n",
      "14           100         log2       30.0  24.592651\n",
      "15           100         log2        NaN  24.592651\n",
      "16           150         auto       10.0  34.933911\n",
      "17           150         auto       20.0  24.453371\n",
      "18           150         auto       30.0  24.403175\n",
      "19           150         auto        NaN  24.403175\n",
      "20           150         log2       10.0  34.933911\n",
      "21           150         log2       20.0  24.453371\n",
      "22           150         log2       30.0  24.403175\n",
      "23           150         log2        NaN  24.403175\n",
      "24           200         auto       10.0  34.680868\n",
      "25           200         auto       20.0  24.173481\n",
      "26           200         auto       30.0  24.196070\n",
      "27           200         auto        NaN  24.196070\n",
      "28           200         log2       10.0  34.680868\n",
      "29           200         log2       20.0  24.173481\n",
      "30           200         log2       30.0  24.196070\n",
      "31           200         log2        NaN  24.196070\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with n_estimators =  200 , best max_features =  auto\n",
      "and best max_depth =  20.0\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_rf = Kfold_tuning_rf(X_undersample_train_rf, y_undersample_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems tuning these 3 hyperparameters does nothing to the result. The optimal result is given by n_estimators=200, max_features='auto' and max_depth=20.\n",
    "Since 200 is the upper bound of our n_estimators_list, more numbers larger than 200 need to be tested.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 24.173480697029717\n"
     ]
    }
   ],
   "source": [
    "fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "mean_loss = []\n",
    "max_features_list = []\n",
    "for i in [200, 250, 300, 350, 400]:\n",
    "    loss_list = []\n",
    "    for k, (train, test) in enumerate(fold.split(X_undersample_train_rf, y_undersample_train_rf)):\n",
    "        rf = RandomForestClassifier(n_estimators=i, max_depth=20, random_state=0)\n",
    "        loss = custom_loss_fuction(rf, X_undersample_train_rf.iloc[train], X_test_rf, y_undersample_train_rf.iloc[train], y_test_rf)\n",
    "        loss_list.append(loss)\n",
    "    mean_loss.append(np.mean(loss_list))\n",
    "    max_features_list.append(i)\n",
    "plt.plot(max_features_list, mean_loss, '-b')\n",
    "plt.ylabel('mean_loss')\n",
    "plt.xlabel('max_features_list')\n",
    "plt.show()\n",
    "\n",
    "min_loss = np.min(mean_loss)\n",
    "min_loss_index = mean_loss.index(min_loss)\n",
    "optimal_max_features = max_features_list[min_loss_index]\n",
    "print optimal_max_features, min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems n_estimators=200 is still the optimal one.\n",
    "Try out min_samples_split ,min_samples_leaf and other number of max_features.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_rf1(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    max_features_range = ['auto', 1, 2, 8]\n",
    "    min_samples_split_range = [2, 4, 8]\n",
    "    min_samples_leaf_range = [1, 2, 4]\n",
    "\n",
    "    max_features_list = []\n",
    "    min_samples_split_list = []\n",
    "    min_samples_leaf_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for max_features in max_features_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'max_features: ', max_features\n",
    "        print '-------------------------------------------'\n",
    "        for min_samples_split in min_samples_split_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'min_samples_split: ', min_samples_split\n",
    "            print '-------------------------------------------'\n",
    "            for min_samples_leaf in min_samples_leaf_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'min_samples_leaf: ', min_samples_leaf\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    rf = RandomForestClassifier(n_estimators=200, max_features=max_features, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_depth=20, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(rf, X_train_data.iloc[train], X_test_rf, y_train_data.iloc[train], y_test_rf)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                max_features_list.append(max_features)\n",
    "                min_samples_split_list.append(min_samples_split)\n",
    "                min_samples_leaf_list.append(min_samples_leaf)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['max_features', 'min_samples_split', 'min_samples_leaf', 'Mean loss'])\n",
    "    results_table['max_features'] = max_features_list\n",
    "    results_table['min_samples_split'] = min_samples_split_list\n",
    "    results_table['min_samples_leaf'] = min_samples_leaf_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_max_features = results_table.loc[results_table['Mean loss'].idxmin()]['max_features']\n",
    "    best_min_samples_split = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_split']\n",
    "    best_min_samples_leaf = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_leaf']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with best max_features = \", best_max_features, \", best min_samples_split = \", best_min_samples_split\n",
    "    print \"and best min_samples_leaf = \", best_min_samples_leaf\n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_rf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.448893259900345\n",
      "Fold  2 : loss =  24.500318416150407\n",
      "Fold  3 : loss =  26.443131543545647\n",
      "Fold  4 : loss =  21.954480416539404\n",
      "Fold  5 : loss =  24.520579849012776\n",
      "\n",
      "Mean loss 24.173480697029717\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.436267868094546\n",
      "Fold  2 : loss =  34.801907818326306\n",
      "Fold  3 : loss =  36.093168089042265\n",
      "Fold  4 : loss =  33.87601062079986\n",
      "Fold  5 : loss =  35.515110402808574\n",
      "\n",
      "Mean loss 34.94449295981431\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.819225502781336\n",
      "Fold  2 : loss =  36.65763509823198\n",
      "Fold  3 : loss =  36.53119325061722\n",
      "Fold  4 : loss =  35.69904745994623\n",
      "Fold  5 : loss =  36.39890885750964\n",
      "\n",
      "Mean loss 36.42120203381728\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  25.25138935698448\n",
      "Fold  2 : loss =  25.03269763016779\n",
      "Fold  3 : loss =  26.739334570519617\n",
      "Fold  4 : loss =  22.76678869060606\n",
      "Fold  5 : loss =  25.03269763016779\n",
      "\n",
      "Mean loss 24.964581575689145\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.436267868094546\n",
      "Fold  2 : loss =  34.801907818326306\n",
      "Fold  3 : loss =  36.093168089042265\n",
      "Fold  4 : loss =  33.87601062079986\n",
      "Fold  5 : loss =  35.515110402808574\n",
      "\n",
      "Mean loss 34.94449295981431\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.819225502781336\n",
      "Fold  2 : loss =  36.65763509823198\n",
      "Fold  3 : loss =  36.53119325061722\n",
      "Fold  4 : loss =  35.69904745994623\n",
      "Fold  5 : loss =  36.39890885750964\n",
      "\n",
      "Mean loss 36.42120203381728\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.83137469473555\n",
      "Fold  2 : loss =  35.60972555276955\n",
      "Fold  3 : loss =  36.04441790451739\n",
      "Fold  4 : loss =  34.425064434391956\n",
      "Fold  5 : loss =  35.24743778787598\n",
      "\n",
      "Mean loss 35.23160407485808\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.3691634036263\n",
      "Fold  2 : loss =  37.04450640995974\n",
      "Fold  3 : loss =  37.7438163467186\n",
      "Fold  4 : loss =  36.65763509823198\n",
      "Fold  5 : loss =  36.5220650406504\n",
      "\n",
      "Mean loss 37.067437259837405\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.819225502781336\n",
      "Fold  2 : loss =  36.65763509823198\n",
      "Fold  3 : loss =  36.53119325061722\n",
      "Fold  4 : loss =  35.69904745994623\n",
      "Fold  5 : loss =  36.39890885750964\n",
      "\n",
      "Mean loss 36.42120203381728\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  21.806009793052482\n",
      "Fold  2 : loss =  21.962636927685068\n",
      "Fold  3 : loss =  24.603898134863698\n",
      "Fold  4 : loss =  20.591256508632497\n",
      "Fold  5 : loss =  22.476609414758272\n",
      "\n",
      "Mean loss 22.288082155798403\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.404652363121144\n",
      "Fold  2 : loss =  33.17785902305744\n",
      "Fold  3 : loss =  34.171463072314936\n",
      "Fold  4 : loss =  33.266364187244356\n",
      "Fold  5 : loss =  34.40190098907358\n",
      "\n",
      "Mean loss 33.88444792696229\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.251397607980365\n",
      "Fold  2 : loss =  36.37676497601797\n",
      "Fold  3 : loss =  36.441149825783974\n",
      "Fold  4 : loss =  35.89036129800954\n",
      "Fold  5 : loss =  37.37879685470815\n",
      "\n",
      "Mean loss 36.6676941125\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  30.848772651581946\n",
      "Fold  2 : loss =  29.78946546315858\n",
      "Fold  3 : loss =  32.65378106499118\n",
      "Fold  4 : loss =  29.99777157410128\n",
      "Fold  5 : loss =  31.523783121029588\n",
      "\n",
      "Mean loss 30.96271477497252\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.404652363121144\n",
      "Fold  2 : loss =  33.17785902305744\n",
      "Fold  3 : loss =  34.171463072314936\n",
      "Fold  4 : loss =  33.266364187244356\n",
      "Fold  5 : loss =  34.40190098907358\n",
      "\n",
      "Mean loss 33.88444792696229\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.251397607980365\n",
      "Fold  2 : loss =  36.37676497601797\n",
      "Fold  3 : loss =  36.441149825783974\n",
      "Fold  4 : loss =  35.89036129800954\n",
      "Fold  5 : loss =  37.37879685470815\n",
      "\n",
      "Mean loss 36.6676941125\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.90324355329442\n",
      "Fold  2 : loss =  35.406114693950116\n",
      "Fold  3 : loss =  36.00536131828261\n",
      "Fold  4 : loss =  34.58987804878049\n",
      "Fold  5 : loss =  35.35448841846909\n",
      "\n",
      "Mean loss 35.45181720655535\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.267423675066624\n",
      "Fold  2 : loss =  35.474480023941354\n",
      "Fold  3 : loss =  35.22704346466542\n",
      "Fold  4 : loss =  34.64509099567983\n",
      "Fold  5 : loss =  36.2153892320039\n",
      "\n",
      "Mean loss 35.36588547827142\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.251397607980365\n",
      "Fold  2 : loss =  36.37676497601797\n",
      "Fold  3 : loss =  36.441149825783974\n",
      "Fold  4 : loss =  35.89036129800954\n",
      "Fold  5 : loss =  37.37879685470815\n",
      "\n",
      "Mean loss 36.6676941125\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.353609756097555\n",
      "Fold  2 : loss =  23.341128915644873\n",
      "Fold  3 : loss =  25.003742415968937\n",
      "Fold  4 : loss =  21.575613174596096\n",
      "Fold  5 : loss =  23.20769608277901\n",
      "\n",
      "Mean loss 23.296358069017295\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.693214038477024\n",
      "Fold  2 : loss =  34.294465805834534\n",
      "Fold  3 : loss =  34.26784528503968\n",
      "Fold  4 : loss =  33.44392793572906\n",
      "Fold  5 : loss =  33.587560815568786\n",
      "\n",
      "Mean loss 34.257402776129815\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.351321138211375\n",
      "Fold  2 : loss =  35.973339140534264\n",
      "Fold  3 : loss =  37.189534141353676\n",
      "Fold  4 : loss =  35.21757268424611\n",
      "Fold  5 : loss =  35.77454224827382\n",
      "\n",
      "Mean loss 36.10126187052384\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  25.960282705099775\n",
      "Fold  2 : loss =  25.829605478704032\n",
      "Fold  3 : loss =  26.50157207480297\n",
      "Fold  4 : loss =  25.553838037818572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  5 : loss =  26.233588635886363\n",
      "\n",
      "Mean loss 26.015777386462343\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.693214038477024\n",
      "Fold  2 : loss =  34.294465805834534\n",
      "Fold  3 : loss =  34.26784528503968\n",
      "Fold  4 : loss =  33.44392793572906\n",
      "Fold  5 : loss =  33.587560815568786\n",
      "\n",
      "Mean loss 34.257402776129815\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.351321138211375\n",
      "Fold  2 : loss =  35.973339140534264\n",
      "Fold  3 : loss =  37.189534141353676\n",
      "Fold  4 : loss =  35.21757268424611\n",
      "Fold  5 : loss =  35.77454224827382\n",
      "\n",
      "Mean loss 36.10126187052384\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.26784528503968\n",
      "Fold  2 : loss =  34.716582057099636\n",
      "Fold  3 : loss =  34.84712790791275\n",
      "Fold  4 : loss =  34.57396760745604\n",
      "Fold  5 : loss =  34.9956367498842\n",
      "\n",
      "Mean loss 34.680231921478466\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.52559281163954\n",
      "Fold  2 : loss =  35.55845289335247\n",
      "Fold  3 : loss =  35.829852915030145\n",
      "Fold  4 : loss =  34.39986656239308\n",
      "Fold  5 : loss =  35.406114693950116\n",
      "\n",
      "Mean loss 35.54397597527307\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.351321138211375\n",
      "Fold  2 : loss =  35.973339140534264\n",
      "Fold  3 : loss =  37.189534141353676\n",
      "Fold  4 : loss =  35.21757268424611\n",
      "Fold  5 : loss =  35.77454224827382\n",
      "\n",
      "Mean loss 36.10126187052384\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.703134281605514\n",
      "Fold  2 : loss =  25.756525347975987\n",
      "Fold  3 : loss =  27.71124680398747\n",
      "Fold  4 : loss =  22.86083742710218\n",
      "Fold  5 : loss =  25.96736630376685\n",
      "\n",
      "Mean loss 25.399822032887602\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.718549698400224\n",
      "Fold  2 : loss =  34.479713863372396\n",
      "Fold  3 : loss =  36.74180377566487\n",
      "Fold  4 : loss =  32.45077548467792\n",
      "Fold  5 : loss =  36.51129016290163\n",
      "\n",
      "Mean loss 34.780426597003405\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.89464811022157\n",
      "Fold  2 : loss =  38.37384032154928\n",
      "Fold  3 : loss =  38.26954057370762\n",
      "Fold  4 : loss =  37.327753253578166\n",
      "Fold  5 : loss =  38.78738878231479\n",
      "\n",
      "Mean loss 38.13063420827429\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.57807916131793\n",
      "Fold  2 : loss =  26.281765389082462\n",
      "Fold  3 : loss =  27.58645528455284\n",
      "Fold  4 : loss =  23.54168659615801\n",
      "Fold  5 : loss =  26.404282644043832\n",
      "\n",
      "Mean loss 25.678453815031013\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.718549698400224\n",
      "Fold  2 : loss =  34.479713863372396\n",
      "Fold  3 : loss =  36.74180377566487\n",
      "Fold  4 : loss =  32.45077548467792\n",
      "Fold  5 : loss =  36.51129016290163\n",
      "\n",
      "Mean loss 34.780426597003405\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.89464811022157\n",
      "Fold  2 : loss =  38.37384032154928\n",
      "Fold  3 : loss =  38.26954057370762\n",
      "Fold  4 : loss =  37.327753253578166\n",
      "Fold  5 : loss =  38.78738878231479\n",
      "\n",
      "Mean loss 38.13063420827429\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  31.05115286970714\n",
      "Fold  2 : loss =  32.07246034919365\n",
      "Fold  3 : loss =  34.086161187698835\n",
      "Fold  4 : loss =  30.49212601626017\n",
      "Fold  5 : loss =  32.735417201540436\n",
      "\n",
      "Mean loss 32.08746352488005\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.64453686464346\n",
      "Fold  2 : loss =  36.74180377566487\n",
      "Fold  3 : loss =  38.37384032154928\n",
      "Fold  4 : loss =  35.66350088101642\n",
      "Fold  5 : loss =  37.830938086303945\n",
      "\n",
      "Mean loss 37.050923985835595\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.89464811022157\n",
      "Fold  2 : loss =  38.37384032154928\n",
      "Fold  3 : loss =  38.26954057370762\n",
      "Fold  4 : loss =  37.327753253578166\n",
      "Fold  5 : loss =  38.78738878231479\n",
      "\n",
      "Mean loss 38.13063420827429\n",
      "\n",
      "   max_features  min_samples_split  min_samples_leaf  Mean loss\n",
      "0          auto                  2                 1  24.173481\n",
      "1          auto                  2                 2  34.944493\n",
      "2          auto                  2                 4  36.421202\n",
      "3          auto                  4                 1  24.964582\n",
      "4          auto                  4                 2  34.944493\n",
      "5          auto                  4                 4  36.421202\n",
      "6          auto                  8                 1  35.231604\n",
      "7          auto                  8                 2  37.067437\n",
      "8          auto                  8                 4  36.421202\n",
      "9             1                  2                 1  22.288082\n",
      "10            1                  2                 2  33.884448\n",
      "11            1                  2                 4  36.667694\n",
      "12            1                  4                 1  30.962715\n",
      "13            1                  4                 2  33.884448\n",
      "14            1                  4                 4  36.667694\n",
      "15            1                  8                 1  35.451817\n",
      "16            1                  8                 2  35.365885\n",
      "17            1                  8                 4  36.667694\n",
      "18            2                  2                 1  23.296358\n",
      "19            2                  2                 2  34.257403\n",
      "20            2                  2                 4  36.101262\n",
      "21            2                  4                 1  26.015777\n",
      "22            2                  4                 2  34.257403\n",
      "23            2                  4                 4  36.101262\n",
      "24            2                  8                 1  34.680232\n",
      "25            2                  8                 2  35.543976\n",
      "26            2                  8                 4  36.101262\n",
      "27            8                  2                 1  25.399822\n",
      "28            8                  2                 2  34.780427\n",
      "29            8                  2                 4  38.130634\n",
      "30            8                  4                 1  25.678454\n",
      "31            8                  4                 2  34.780427\n",
      "32            8                  4                 4  38.130634\n",
      "33            8                  8                 1  32.087464\n",
      "34            8                  8                 2  37.050924\n",
      "35            8                  8                 4  38.130634\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with best max_features =  1 , best min_samples_split =  2\n",
      "and best min_samples_leaf =  1\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_rf1 = Kfold_tuning_rf1(X_undersample_train_rf, y_undersample_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* optimal max_features = 1, the values smaller than 1 will not be discussed here anymore. the optimal min_samples_split and min_samples_leaf are all default value.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Random Forest Model\n",
    "* n_estimators=200, max_features=1, max_depth=20; min_samples_split and min_samples_leaf equals to default value.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 70 proportion\n",
      "the recall for this model is : 0.943089430894309\n",
      "The accuracy is : 0.9996348404358039\n",
      "TP 464\n",
      "TN 284239\n",
      "FP 76\n",
      "FN 28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.86      0.94      0.90       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n",
      "The loss is :  19.37591869918699\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 70 proportion\"\n",
    "optimal_rf = RandomForestClassifier(n_estimators=200, max_features=1, max_depth=20, random_state=0)\n",
    "prediction_algorithms(optimal_rf, X_undersample_train_rf, X_test_rf, y_undersample_train_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Tree\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.991452991453\n",
      "the fraud transacation proportion is : 0.00854700854701\n",
      "total number of record in resampled data is: 57564\n"
     ]
    }
   ],
   "source": [
    "df_gbt = df.loc[:, ['V11', 'V17', 'V14', 'V10', 'V12', 'V4', 'Class']]\n",
    "undersample_data_gbt = undersample(df_gbt, normal_indices,fraud_indices, 116)\n",
    "X_undersample_gbt = undersample_data_gbt.iloc[:, undersample_data_gbt.columns != \"Class\"]\n",
    "y_undersample_gbt = undersample_data_gbt.iloc[:, undersample_data_gbt.columns == \"Class\"]\n",
    "X_undersample_train_gbt, X_undersample_test_gbt, y_undersample_train_gbt, y_undersample_test_gbt = train_test_split(X_undersample_gbt, y_undersample_gbt, random_state=0)\n",
    "X_test_gbt = df_gbt.iloc[:, df_gbt.columns != \"Class\"]\n",
    "y_test_gbt = df_gbt.iloc[:, df_gbt.columns == \"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed learning rate, tuning n_estimators, max_depth, min_samples_split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    n_estimators_range = [100, 200, 300]\n",
    "    max_depth_range = [3, 5, 7, 9]\n",
    "    min_samples_split_range = [2, 4, 8, 16]\n",
    "    n_estimators_list = []\n",
    "    max_depth_list = []\n",
    "    min_samples_split_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'n_estimators: ', n_estimators\n",
    "        print '-------------------------------------------'\n",
    "        for max_depth in max_depth_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'max_depth: ', max_depth\n",
    "            print '-------------------------------------------'\n",
    "            for min_samples_split in min_samples_split_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'min_samples_split: ', min_samples_split\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    gbt = GradientBoostingClassifier(n_estimators=n_estimators,max_depth=max_depth, min_samples_split=min_samples_split, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                n_estimators_list.append(n_estimators)\n",
    "                max_depth_list.append(max_depth)\n",
    "                min_samples_split_list.append(min_samples_split)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['n_estimators', 'max_depth', 'min_samples_split', 'Mean loss'])\n",
    "    results_table['n_estimators'] = n_estimators_list\n",
    "    results_table['max_depth'] = max_depth_list\n",
    "    results_table['min_samples_split'] = min_samples_split_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['n_estimators']\n",
    "    best_max_depth = results_table.loc[results_table['Mean loss'].idxmin()]['max_depth']\n",
    "    best_min_samples_split = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_split']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with n_estimators = \", best_n_estimators, \"and best max_depth = \", best_max_depth\n",
    "    print \"and best min_samples_split = \", best_min_samples_split \n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "n_estimators:  100\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.32608048780488\n",
      "Fold  2 : loss =  46.710879517703574\n",
      "Fold  3 : loss =  50.15703668591456\n",
      "Fold  4 : loss =  41.38370921761165\n",
      "Fold  5 : loss =  43.09768604473311\n",
      "\n",
      "Mean loss 45.33507839075356\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.32608048780488\n",
      "Fold  2 : loss =  46.39197339246119\n",
      "Fold  3 : loss =  49.47865984788881\n",
      "Fold  4 : loss =  40.81816004059001\n",
      "Fold  5 : loss =  43.09768604473311\n",
      "\n",
      "Mean loss 45.022511962695596\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.32608048780488\n",
      "Fold  2 : loss =  45.79630952380953\n",
      "Fold  3 : loss =  49.36370137773234\n",
      "Fold  4 : loss =  42.705700813008136\n",
      "Fold  5 : loss =  41.49008674506389\n",
      "\n",
      "Mean loss 44.93637578948376\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.39349466131738\n",
      "Fold  2 : loss =  43.77496278796349\n",
      "Fold  3 : loss =  51.83638765914153\n",
      "Fold  4 : loss =  40.7189263111144\n",
      "Fold  5 : loss =  43.614646654158854\n",
      "\n",
      "Mean loss 45.06768361473914\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.23491526604373\n",
      "Fold  2 : loss =  43.641858704793954\n",
      "Fold  3 : loss =  49.36176934037382\n",
      "Fold  4 : loss =  48.471023605409314\n",
      "Fold  5 : loss =  42.44749560975609\n",
      "\n",
      "Mean loss 46.23141250527538\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.685236579733\n",
      "Fold  2 : loss =  44.6210783055199\n",
      "Fold  3 : loss =  49.521436407172295\n",
      "Fold  4 : loss =  51.94565241797412\n",
      "Fold  5 : loss =  42.30302106430155\n",
      "\n",
      "Mean loss 47.61528495494017\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.95884146341464\n",
      "Fold  2 : loss =  39.0591122860879\n",
      "Fold  3 : loss =  44.52856043086348\n",
      "Fold  4 : loss =  49.89888572936209\n",
      "Fold  5 : loss =  43.4662431681091\n",
      "\n",
      "Mean loss 45.18232861556744\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  44.00416282689275\n",
      "Fold  2 : loss =  43.44149530956849\n",
      "Fold  3 : loss =  48.35139128671571\n",
      "Fold  4 : loss =  46.13393462919401\n",
      "Fold  5 : loss =  45.06631533101046\n",
      "\n",
      "Mean loss 45.39945987667629\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.35963015521064\n",
      "Fold  2 : loss =  46.16608283328856\n",
      "Fold  3 : loss =  50.59973051977711\n",
      "Fold  4 : loss =  49.07681407372327\n",
      "Fold  5 : loss =  46.93927634325243\n",
      "\n",
      "Mean loss 48.0283067850504\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.301857650552556\n",
      "Fold  2 : loss =  46.768482712517574\n",
      "Fold  3 : loss =  50.34191677001517\n",
      "Fold  4 : loss =  48.780548410938664\n",
      "Fold  5 : loss =  46.5732046737955\n",
      "\n",
      "Mean loss 48.15320204356389\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.0053831704382\n",
      "Fold  2 : loss =  46.58313097580854\n",
      "Fold  3 : loss =  48.984910813195896\n",
      "Fold  4 : loss =  48.34819675603704\n",
      "Fold  5 : loss =  44.03045060296362\n",
      "\n",
      "Mean loss 46.99041446368865\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.38023705605477\n",
      "Fold  2 : loss =  45.673440360979626\n",
      "Fold  3 : loss =  49.521436407172295\n",
      "Fold  4 : loss =  44.803422256097555\n",
      "Fold  5 : loss =  43.62473313955938\n",
      "\n",
      "Mean loss 46.40065384397273\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.817449941928\n",
      "Fold  2 : loss =  48.21261622581015\n",
      "Fold  3 : loss =  48.95290392496475\n",
      "Fold  4 : loss =  46.00118090742198\n",
      "Fold  5 : loss =  43.5069756097561\n",
      "\n",
      "Mean loss 47.2982253219762\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.6473558384811\n",
      "Fold  2 : loss =  41.78789505591461\n",
      "Fold  3 : loss =  49.65776422764229\n",
      "Fold  4 : loss =  43.54528975609756\n",
      "Fold  5 : loss =  41.64320835671144\n",
      "\n",
      "Mean loss 45.0563026469694\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  46.40880375904856\n",
      "Fold  2 : loss =  45.655678299871425\n",
      "Fold  3 : loss =  47.41858153533459\n",
      "Fold  4 : loss =  47.521609756097554\n",
      "Fold  5 : loss =  41.791209267856765\n",
      "\n",
      "Mean loss 45.759176523641784\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.98247667845229\n",
      "Fold  2 : loss =  40.3811512061842\n",
      "Fold  3 : loss =  47.6521422180729\n",
      "Fold  4 : loss =  45.54365853658537\n",
      "Fold  5 : loss =  46.03078647733326\n",
      "\n",
      "Mean loss 45.118043023325605\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  200\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.521609756097554\n",
      "Fold  2 : loss =  46.589297991159526\n",
      "Fold  3 : loss =  47.5301599883856\n",
      "Fold  4 : loss =  41.24738456182905\n",
      "Fold  5 : loss =  41.67944613821139\n",
      "\n",
      "Mean loss 44.91357968713662\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.521609756097554\n",
      "Fold  2 : loss =  45.84393048074402\n",
      "Fold  3 : loss =  46.9997668185473\n",
      "Fold  4 : loss =  42.524334444953034\n",
      "Fold  5 : loss =  41.83400119361305\n",
      "\n",
      "Mean loss 44.94472853879099\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.521609756097554\n",
      "Fold  2 : loss =  49.12496459480723\n",
      "Fold  3 : loss =  47.00440059127864\n",
      "Fold  4 : loss =  41.99894308943089\n",
      "Fold  5 : loss =  40.5880774748924\n",
      "\n",
      "Mean loss 45.24759910130135\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.45229363940699\n",
      "Fold  2 : loss =  45.79751215502291\n",
      "Fold  3 : loss =  50.59973051977711\n",
      "Fold  4 : loss =  43.112383949645945\n",
      "Fold  5 : loss =  44.40651598629593\n",
      "\n",
      "Mean loss 46.473687250029776\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  46.497112457708425\n",
      "Fold  2 : loss =  42.13584212044486\n",
      "Fold  3 : loss =  48.76681502650293\n",
      "Fold  4 : loss =  47.1113507021434\n",
      "Fold  5 : loss =  41.41968484888653\n",
      "\n",
      "Mean loss 45.18616103113723\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.02768521162963\n",
      "Fold  2 : loss =  43.906633712461655\n",
      "Fold  3 : loss =  48.905135952507635\n",
      "Fold  4 : loss =  50.84055883611469\n",
      "Fold  5 : loss =  41.34477152812343\n",
      "\n",
      "Mean loss 46.80495704816741\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.416179050421604\n",
      "Fold  2 : loss =  38.08741202049289\n",
      "Fold  3 : loss =  44.00282870759742\n",
      "Fold  4 : loss =  49.19540243902439\n",
      "Fold  5 : loss =  42.77972931611669\n",
      "\n",
      "Mean loss 44.49631030673059\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  42.605503949347394\n",
      "Fold  2 : loss =  42.89957980538355\n",
      "Fold  3 : loss =  47.30095699872662\n",
      "Fold  4 : loss =  45.27691298190401\n",
      "Fold  5 : loss =  44.25160975609756\n",
      "\n",
      "Mean loss 44.466912698291836\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.060109756097546\n",
      "Fold  2 : loss =  46.16608283328856\n",
      "Fold  3 : loss =  50.59973051977711\n",
      "Fold  4 : loss =  49.017706882415126\n",
      "Fold  5 : loss =  46.87863778845866\n",
      "\n",
      "Mean loss 47.9444535560074\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.185585551293684\n",
      "Fold  2 : loss =  46.64917102734664\n",
      "Fold  3 : loss =  50.34191677001517\n",
      "Fold  4 : loss =  48.780548410938664\n",
      "Fold  5 : loss =  46.511666115304806\n",
      "\n",
      "Mean loss 48.093777574979796\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.79751215502291\n",
      "Fold  2 : loss =  46.40037158960548\n",
      "Fold  3 : loss =  48.1564799900643\n",
      "Fold  4 : loss =  48.10070524907245\n",
      "Fold  5 : loss =  43.212659788999986\n",
      "\n",
      "Mean loss 46.33354575455303\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.80452007681065\n",
      "Fold  2 : loss =  44.46342626324998\n",
      "Fold  3 : loss =  48.657006333900554\n",
      "Fold  4 : loss =  44.11183666902632\n",
      "Fold  5 : loss =  42.44018144618533\n",
      "\n",
      "Mean loss 45.49539415783457\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.76434317501071\n",
      "Fold  2 : loss =  48.21261622581015\n",
      "Fold  3 : loss =  48.95290392496475\n",
      "Fold  4 : loss =  46.00118090742198\n",
      "Fold  5 : loss =  43.5069756097561\n",
      "\n",
      "Mean loss 47.28760396859274\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.6473558384811\n",
      "Fold  2 : loss =  41.64317767863927\n",
      "Fold  3 : loss =  49.65776422764229\n",
      "Fold  4 : loss =  43.47362043700544\n",
      "Fold  5 : loss =  41.64320835671144\n",
      "\n",
      "Mean loss 45.01302530769591\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  46.34834552845528\n",
      "Fold  2 : loss =  45.71858512316467\n",
      "Fold  3 : loss =  47.477181994749124\n",
      "Fold  4 : loss =  47.27493978091145\n",
      "Fold  5 : loss =  41.71813335515906\n",
      "\n",
      "Mean loss 45.70743715648793\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.98247667845229\n",
      "Fold  2 : loss =  40.152330623306234\n",
      "Fold  3 : loss =  47.53564192548401\n",
      "Fold  4 : loss =  45.47722929337262\n",
      "Fold  5 : loss =  45.59261460170712\n",
      "\n",
      "Mean loss 44.94805862446445\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  300\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.74821942601051\n",
      "Fold  2 : loss =  46.34834552845528\n",
      "Fold  3 : loss =  47.5939618324327\n",
      "Fold  4 : loss =  41.107701434786016\n",
      "Fold  5 : loss =  41.91095806767197\n",
      "\n",
      "Mean loss 45.141837257871295\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.74821942601051\n",
      "Fold  2 : loss =  44.69291651190624\n",
      "Fold  3 : loss =  46.58313097580854\n",
      "Fold  4 : loss =  41.93674796747967\n",
      "Fold  5 : loss =  41.987702353681\n",
      "\n",
      "Mean loss 44.7897434469772\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.74821942601051\n",
      "Fold  2 : loss =  48.66078466562711\n",
      "Fold  3 : loss =  46.70422764227642\n",
      "Fold  4 : loss =  41.48762961482074\n",
      "Fold  5 : loss =  40.04770158507341\n",
      "\n",
      "Mean loss 45.12971258676164\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.94096874607272\n",
      "Fold  2 : loss =  45.54875358902288\n",
      "Fold  3 : loss =  50.18582545213208\n",
      "Fold  4 : loss =  41.55359342892881\n",
      "Fold  5 : loss =  44.476525623699516\n",
      "\n",
      "Mean loss 45.941133367971204\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  46.497112457708425\n",
      "Fold  2 : loss =  42.13584212044486\n",
      "Fold  3 : loss =  48.76681502650293\n",
      "Fold  4 : loss =  47.1113507021434\n",
      "Fold  5 : loss =  41.41968484888653\n",
      "\n",
      "Mean loss 45.18616103113723\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.02768521162963\n",
      "Fold  2 : loss =  43.906633712461655\n",
      "Fold  3 : loss =  48.905135952507635\n",
      "Fold  4 : loss =  50.84055883611469\n",
      "Fold  5 : loss =  41.34477152812343\n",
      "\n",
      "Mean loss 46.80495704816741\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.416179050421604\n",
      "Fold  2 : loss =  37.589014825442376\n",
      "Fold  3 : loss =  43.80289899937461\n",
      "Fold  4 : loss =  49.19540243902439\n",
      "Fold  5 : loss =  42.77972931611669\n",
      "\n",
      "Mean loss 44.35664492607593\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.67382775571942\n",
      "Fold  2 : loss =  41.05808409076049\n",
      "Fold  3 : loss =  47.0053831704382\n",
      "Fold  4 : loss =  44.87159155063592\n",
      "Fold  5 : loss =  43.88722732135216\n",
      "\n",
      "Mean loss 43.89922277778123\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.060109756097546\n",
      "Fold  2 : loss =  46.16608283328856\n",
      "Fold  3 : loss =  50.59973051977711\n",
      "Fold  4 : loss =  49.017706882415126\n",
      "Fold  5 : loss =  46.87863778845866\n",
      "\n",
      "Mean loss 47.9444535560074\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.185585551293684\n",
      "Fold  2 : loss =  46.64917102734664\n",
      "Fold  3 : loss =  50.34191677001517\n",
      "Fold  4 : loss =  48.780548410938664\n",
      "Fold  5 : loss =  46.511666115304806\n",
      "\n",
      "Mean loss 48.093777574979796\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.79751215502291\n",
      "Fold  2 : loss =  46.40037158960548\n",
      "Fold  3 : loss =  48.1564799900643\n",
      "Fold  4 : loss =  48.10070524907245\n",
      "Fold  5 : loss =  43.212659788999986\n",
      "\n",
      "Mean loss 46.33354575455303\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.80452007681065\n",
      "Fold  2 : loss =  44.46342626324998\n",
      "Fold  3 : loss =  48.657006333900554\n",
      "Fold  4 : loss =  44.11183666902632\n",
      "Fold  5 : loss =  42.44018144618533\n",
      "\n",
      "Mean loss 45.49539415783457\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.76434317501071\n",
      "Fold  2 : loss =  48.21261622581015\n",
      "Fold  3 : loss =  48.95290392496475\n",
      "Fold  4 : loss =  46.00118090742198\n",
      "Fold  5 : loss =  43.5069756097561\n",
      "\n",
      "Mean loss 47.28760396859274\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.6473558384811\n",
      "Fold  2 : loss =  41.64317767863927\n",
      "Fold  3 : loss =  49.65776422764229\n",
      "Fold  4 : loss =  43.47362043700544\n",
      "Fold  5 : loss =  41.64320835671144\n",
      "\n",
      "Mean loss 45.01302530769591\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  46.34834552845528\n",
      "Fold  2 : loss =  45.71858512316467\n",
      "Fold  3 : loss =  47.477181994749124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 : loss =  47.27493978091145\n",
      "Fold  5 : loss =  41.71813335515906\n",
      "\n",
      "Mean loss 45.70743715648793\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.98247667845229\n",
      "Fold  2 : loss =  40.152330623306234\n",
      "Fold  3 : loss =  47.53564192548401\n",
      "Fold  4 : loss =  45.47722929337262\n",
      "Fold  5 : loss =  45.59261460170712\n",
      "\n",
      "Mean loss 44.94805862446445\n",
      "\n",
      "    n_estimators  max_depth  min_samples_split  Mean loss\n",
      "0            100          3                  2  45.335078\n",
      "1            100          3                  4  45.022512\n",
      "2            100          3                  8  44.936376\n",
      "3            100          3                 16  45.067684\n",
      "4            100          5                  2  46.231413\n",
      "5            100          5                  4  47.615285\n",
      "6            100          5                  8  45.182329\n",
      "7            100          5                 16  45.399460\n",
      "8            100          7                  2  48.028307\n",
      "9            100          7                  4  48.153202\n",
      "10           100          7                  8  46.990414\n",
      "11           100          7                 16  46.400654\n",
      "12           100          9                  2  47.298225\n",
      "13           100          9                  4  45.056303\n",
      "14           100          9                  8  45.759177\n",
      "15           100          9                 16  45.118043\n",
      "16           200          3                  2  44.913580\n",
      "17           200          3                  4  44.944729\n",
      "18           200          3                  8  45.247599\n",
      "19           200          3                 16  46.473687\n",
      "20           200          5                  2  45.186161\n",
      "21           200          5                  4  46.804957\n",
      "22           200          5                  8  44.496310\n",
      "23           200          5                 16  44.466913\n",
      "24           200          7                  2  47.944454\n",
      "25           200          7                  4  48.093778\n",
      "26           200          7                  8  46.333546\n",
      "27           200          7                 16  45.495394\n",
      "28           200          9                  2  47.287604\n",
      "29           200          9                  4  45.013025\n",
      "30           200          9                  8  45.707437\n",
      "31           200          9                 16  44.948059\n",
      "32           300          3                  2  45.141837\n",
      "33           300          3                  4  44.789743\n",
      "34           300          3                  8  45.129713\n",
      "35           300          3                 16  45.941133\n",
      "36           300          5                  2  45.186161\n",
      "37           300          5                  4  46.804957\n",
      "38           300          5                  8  44.356645\n",
      "39           300          5                 16  43.899223\n",
      "40           300          7                  2  47.944454\n",
      "41           300          7                  4  48.093778\n",
      "42           300          7                  8  46.333546\n",
      "43           300          7                 16  45.495394\n",
      "44           300          9                  2  47.287604\n",
      "45           300          9                  4  45.013025\n",
      "46           300          9                  8  45.707437\n",
      "47           300          9                 16  44.948059\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with n_estimators =  300.0 and best max_depth =  5.0\n",
      "and best min_samples_split =  16.0\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt = Kfold_tuning_gbt(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal n_estimators and max_depth are 300 and 5.\n",
    "* The optimal min_samples_split could not be decided by the above grid search because 16 is the upper bound of our max_depth_range. Thus, more numbers larger than 16 should be tried.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 42.96893443012151\n"
     ]
    }
   ],
   "source": [
    "fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "mean_loss = []\n",
    "min_samples_split_list = []\n",
    "for i in [16, 32, 48, 64, 80]:\n",
    "    loss_list = []\n",
    "    for k, (train, test) in enumerate(fold.split(X_undersample_train_gbt, y_undersample_train_gbt)):\n",
    "        gbt = GradientBoostingClassifier(n_estimators=300, max_depth=5, min_samples_split=i, random_state=0)\n",
    "        loss = custom_loss_fuction(gbt, X_undersample_train_gbt.iloc[train], X_test_gbt, y_undersample_train_gbt.iloc[train], y_test_gbt)\n",
    "        loss_list.append(loss)\n",
    "    mean_loss.append(np.mean(loss_list))\n",
    "    min_samples_split_list.append(i)\n",
    "plt.plot(min_samples_split_list, mean_loss, '-b')\n",
    "plt.ylabel('mean_loss')\n",
    "plt.xlabel('min_samples_split_list')\n",
    "plt.show()\n",
    "\n",
    "min_loss = np.min(mean_loss)\n",
    "min_loss_index = mean_loss.index(min_loss)\n",
    "optimal_min_samples_split = min_samples_split_list[min_loss_index]\n",
    "print optimal_min_samples_split, min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, we got optimal n_estimators=300, max_depth=5, min_samples_split=48.\n",
    "#### Tuning subsample and making models with lower learning rate.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt1(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    subsample_range = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "    subsample_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for subsample in subsample_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'subsample: ', subsample\n",
    "        print '-------------------------------------------'\n",
    "        loss_list = []\n",
    "        for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "            gbt = GradientBoostingClassifier(n_estimators=300,max_depth=5, min_samples_split=48, subsample=subsample, random_state=0)\n",
    "\n",
    "            loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "            loss_list.append(loss)\n",
    "            print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "        j += 1\n",
    "        print ''\n",
    "        print 'Mean loss', np.mean(loss_list)\n",
    "        print ''\n",
    "        subsample_list.append(subsample)\n",
    "        mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Subsample','Mean loss'])\n",
    "    results_table['Subsample'] = subsample_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "\n",
    "    best_subsample = results_table.loc[results_table['Mean loss'].idxmin()]['Subsample']\n",
    "\n",
    "    print results_table\n",
    "    print  ''\n",
    "\n",
    "    print '*******************************************************************'\n",
    "    print \"Best model to choose from cross validation is with subsample = \", best_subsample\n",
    "    print '*******************************************************************'\n",
    "\n",
    "    return Kfold_tuning_gbt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "subsample:  0.6\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  97.05728418287352\n",
      "Fold  2 : loss =  80.71442188132289\n",
      "Fold  3 : loss =  71.44273769627009\n",
      "Fold  4 : loss =  119.66814811655114\n",
      "Fold  5 : loss =  97.06228998643448\n",
      "\n",
      "Mean loss 93.18897637269042\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.65\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  73.47108818011257\n",
      "Fold  2 : loss =  154.97790708355305\n",
      "Fold  3 : loss =  50.998248337028826\n",
      "Fold  4 : loss =  74.79161707317073\n",
      "Fold  5 : loss =  81.13679255179649\n",
      "\n",
      "Mean loss 87.07513064513232\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.7\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  100.25728214568068\n",
      "Fold  2 : loss =  46.05305279106385\n",
      "Fold  3 : loss =  80.8283479759375\n",
      "Fold  4 : loss =  97.83547902776259\n",
      "Fold  5 : loss =  70.27910106794\n",
      "\n",
      "Mean loss 79.05065260167693\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.75\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  97.30351730242771\n",
      "Fold  2 : loss =  44.82056054771074\n",
      "Fold  3 : loss =  51.37340155574425\n",
      "Fold  4 : loss =  56.99213297050326\n",
      "Fold  5 : loss =  47.08431966156969\n",
      "\n",
      "Mean loss 59.51478640759113\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.3263858492639\n",
      "Fold  2 : loss =  41.568902439024384\n",
      "Fold  3 : loss =  93.39661329080108\n",
      "Fold  4 : loss =  46.86874309544575\n",
      "Fold  5 : loss =  52.838321138211384\n",
      "\n",
      "Mean loss 57.3997931625493\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.85\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.89888572936209\n",
      "Fold  2 : loss =  93.5831881897523\n",
      "Fold  3 : loss =  52.230701509872254\n",
      "Fold  4 : loss =  44.54635078248016\n",
      "Fold  5 : loss =  46.469114889324075\n",
      "\n",
      "Mean loss 57.34564822015818\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.9\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.82849757793536\n",
      "Fold  2 : loss =  44.29878786946761\n",
      "Fold  3 : loss =  45.920973509735106\n",
      "Fold  4 : loss =  42.434225623773486\n",
      "Fold  5 : loss =  92.84298858214885\n",
      "\n",
      "Mean loss 53.46509463261208\n",
      "\n",
      "   Subsample  Mean loss\n",
      "0       0.60  93.188976\n",
      "1       0.65  87.075131\n",
      "2       0.70  79.050653\n",
      "3       0.75  59.514786\n",
      "4       0.80  57.399793\n",
      "5       0.85  57.345648\n",
      "6       0.90  53.465095\n",
      "\n",
      "*******************************************************************\n",
      "Best model to choose from cross validation is with subsample =  0.9\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt1 = Kfold_tuning_gbt1(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When subsample = 0.9, the mean loss is 53.46 which is larger than the one(42.97) with subsample=1(default). Thus, the optimal subsample=default(1.0).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt2(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    learning_rate_range = [0.1, 0.05, 0.01, 0.005]\n",
    "    n_estimators_range = [300, 600, 3000, 6000]\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0,4):\n",
    "        print '-------------------------------------------'\n",
    "        print 'learning_rate: ', learning_rate_range[i]\n",
    "        print 'n_estimators: ', n_estimators_range[i]\n",
    "        print '-------------------------------------------'\n",
    "        loss_list = []\n",
    "        for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "            gbt = GradientBoostingClassifier(learning_rate=learning_rate_range[i], n_estimators=n_estimators_range[i],max_depth=5, min_samples_split=48, random_state=0)\n",
    "\n",
    "            loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "            loss_list.append(loss)\n",
    "            print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "        j += 1\n",
    "        print ''\n",
    "        print 'Mean loss', np.mean(loss_list)\n",
    "        print ''\n",
    "        mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Learning_rate', 'N_estimators', 'Mean loss'])\n",
    "    results_table['Learning_rate'] = learning_rate_range\n",
    "    results_table['N_estimators'] = n_estimators_range\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "    \n",
    "    best_learning_rate = results_table.loc[results_table['Mean loss'].idxmin()]['Learning_rate']\n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['N_estimators']\n",
    "\n",
    "    print results_table\n",
    "    print  ''\n",
    "\n",
    "    print \"***************************************************************************\"\n",
    "    print \"Best model to choose from cross validation is with best learning rate = \", best_learning_rate,\n",
    "    print \"and corresponding n_estimators = \", best_n_estimators\n",
    "    print \"***************************************************************************\"\n",
    "\n",
    "    return Kfold_tuning_gbt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "learning_rate:  0.1\n",
      "n_estimators:  300\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  43.641858704793954\n",
      "Fold  2 : loss =  42.71008949474312\n",
      "Fold  3 : loss =  45.23431593779779\n",
      "Fold  4 : loss =  41.781050904763504\n",
      "Fold  5 : loss =  41.477357108509196\n",
      "\n",
      "Mean loss 42.96893443012151\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.05\n",
      "n_estimators:  600\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.275524714787075\n",
      "Fold  2 : loss =  39.24052428889941\n",
      "Fold  3 : loss =  44.52856043086348\n",
      "Fold  4 : loss =  41.386164541118355\n",
      "Fold  5 : loss =  36.175752395470376\n",
      "\n",
      "Mean loss 40.521305274227736\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.01\n",
      "n_estimators:  3000\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  32.01255554713932\n",
      "Fold  2 : loss =  31.19523386847793\n",
      "Fold  3 : loss =  30.652787195448195\n",
      "Fold  4 : loss =  28.2118383795907\n",
      "Fold  5 : loss =  32.58628181749107\n",
      "\n",
      "Mean loss 30.931739361629447\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.005\n",
      "n_estimators:  6000\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.85596282166688\n",
      "Fold  2 : loss =  27.347638848901134\n",
      "Fold  3 : loss =  25.44436220428062\n",
      "Fold  4 : loss =  26.066097846742974\n",
      "Fold  5 : loss =  25.68802434811913\n",
      "\n",
      "Mean loss 25.880417213942145\n",
      "\n",
      "   Learning_rate  N_estimators  Mean loss\n",
      "0          0.100           300  42.968934\n",
      "1          0.050           600  40.521305\n",
      "2          0.010          3000  30.931739\n",
      "3          0.005          6000  25.880417\n",
      "\n",
      "***************************************************************************\n",
      "Best model to choose from cross validation is with best learning rate =  0.005 ***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt2 = Kfold_tuning_gbt2(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal GBT Model\n",
    "* leanring_rate = 0.005, n_estimators = 6000, max_depth=5, min_samples_split=48; subsample equals to default.\n",
    "* Though keep decresing the learning rate might give us a better result, the slow learning rate will consume a lot of time and will not improve the model performance quite much. Even 0.005 will slow down the calculation here. Thus, the learning rate smaller than 0.005 will not be discussed in this project.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 116 proportion\n",
      "the recall for this model is : 0.9613821138211383\n",
      "The accuracy is : 0.999515461347509\n",
      "TP 473\n",
      "TN 284196\n",
      "FP 119\n",
      "FN 19\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.80      0.96      0.87       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n",
      "The loss is :  22.463499368270703\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 116 proportion\"\n",
    "optimal_gbt = GradientBoostingClassifier(learning_rate=0.005, n_estimators=6000, max_depth=5, min_samples_split=48, random_state=0)\n",
    "prediction_algorithms(optimal_gbt, X_undersample_train_gbt, X_test_gbt, y_undersample_train_gbt, y_test_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of model selection\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performances without resampling, pre-precessing or tunning hyperparameters:\n",
    "\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|\n",
    "|   Logistic Regression  |    0.81   |  0.60  | 0.69 |   0.99   |    65.62    |\n",
    "| Support Vector Machine |    1.00   |  0.03  | 0.05 |   0.99   |    118.46   |\n",
    "|      *Random Forest    |    0.94   |  0.75  | 0.83 |   0.99   |    35.83    | \n",
    "| Gradient Boosting Tree |    0.80   |  0.47  | 0.60 |   0.99   |    82.38   |\n",
    "\n",
    "#### Optimal model performances after pre-processing and selection of optimal proportion:\n",
    "\n",
    "* All the models perform better after selecting a appropriate proportion for undersampling especially for SVM and GBT.\n",
    "\n",
    "* Random Forest is the best among the 4 models.\n",
    "\n",
    "* Models in this part and the next part are trained on the undersampled dataset and tested on the WHOLE DATA SET. Thus, the performances here might be worse than the 1st part.\n",
    "\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss | Diff vs 1st table |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|:-----------------:|\n",
    "|   Logistic Regression  |    0.60   |  0.51  | 0.55 |   0.99   |    95.15    |        29.53      |\n",
    "| Support Vector Machine |    1.00   |  0.77  | 0.87 |   0.99   |    28.09    |       -90.37      |\n",
    "|      *Random Forest    |    0.82   |  0.96  | 0.88 |   0.99   |    20.78    |       -15.05      |\n",
    "| Gradient Boosting Tree |    0.68   |  0.93  | 0.79 |   0.99   |    36.80    |       -45.58      |\n",
    "\n",
    "#### Optimal model performances after tunning hyperparameters:\n",
    "\n",
    "* All the models have got improvements from hyperparameters' tunning especially for Logistic Regression and Gradient Boosting Tree.\n",
    "\n",
    "* In conclusion, SVM and Random Forest are more sensitive to the proportion of undersampling compared to tunning hyperparameters. Logistic Reregssion and GBT are sensetive to both of them.\n",
    "\n",
    "* Random Forest is still the best amoung the 4 models. Actually the improvement after tunning hyperparameters is trivial for Support Vector Machine (non-linear) and Random Forest. Anyway, better than nothing. It still saves about $ 1.09 per (fraud + misclassification normal) transaction for us.\n",
    "\n",
    "* SVM has a 100% precision which means if a transaction is classified as fraud by SVM then it must be a fraud. Thus, a typical predication process would be like this: predict using SVM first, if result is fraud then classify the transaction as fraud. Otherwise, predict agian using Random Forest and return the prediction result. By doing this, we could increase the recall rate as far as possible without doing any harm to precision.\n",
    "\n",
    "* The optimal SVM and Random Forest models are:\n",
    "> SVC(C=1, gamma=0.1)\n",
    "<br>RandomForestClassifier(n_estimators=200, max_features=1, max_depth=20, min_smaples_split=2, min_samples_leaf=1)\n",
    "    \n",
    "\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss | Diff vs 1st table | Diff vs 2nd table |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|:-----------------:|:-----------------:|\n",
    "|   Logistic Regression  |    0.79   |  0.78  | 0.79 |   0.99   |    45.41    |       -20.21      |       -49.74      |\n",
    "| *Support Vector Machine|    1.00   |  0.78  | 0.88 |   0.99   |    26.87    |       -91.59      |       -1.22       |\n",
    "|      *Random Forest    |    0.86   |  0.94  | 0.90 |   0.99   |    19.69    |       -16.14      |       -1.09       |\n",
    "| Gradient Boosting Tree |    0.80   |  0.96  | 0.87 |   0.99   |    22.54    |       -59.84      |       -14.26      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
