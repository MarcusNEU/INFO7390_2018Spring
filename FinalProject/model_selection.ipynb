{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from __future__ import division\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/creditcard.csv')\n",
    "df_fe = df.drop(['V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_normal_transacation = len(df_fe[df_fe[\"Class\"]==0]) # normal transaction are repersented by 0\n",
    "count_fraud_transacation = len(df_fe[df_fe[\"Class\"]==1]) # fraud by 1\n",
    "fraud_indices = np.array(df_fe[df_fe.Class==1].index)\n",
    "normal_indices = np.array(df_fe[df_fe.Class==0].index)\n",
    "\n",
    "#now let us a define a function for make undersample data with different proportion\n",
    "#different proportion means with different proportion of normal classes of data\n",
    "\n",
    "def undersample(df, normal_indices, fraud_indices, multiple): # multiple denote the normal data = multiple * fraud data\n",
    "    normal_indices_undersample = np.array(np.random.choice(normal_indices,(multiple*count_fraud_transacation),replace=False))\n",
    "    undersample_data = np.concatenate([fraud_indices, normal_indices_undersample])\n",
    "    undersample_data = df.iloc[undersample_data,:]\n",
    "    \n",
    "    print \"the normal transacation proportion is :\", len(undersample_data[undersample_data.Class==0])/len(undersample_data)\n",
    "    print \"the fraud transacation proportion is :\", len(undersample_data[undersample_data.Class==1])/len(undersample_data)\n",
    "    print \"total number of record in resampled data is:\",len(undersample_data)\n",
    "    return(undersample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_algorithms(model, features_train, features_test, labels_train, labels_test):\n",
    "    model.fit(features_train, labels_train.values.ravel())\n",
    "    pred = model.predict(features_test)\n",
    "    cm = confusion_matrix(labels_test,pred)\n",
    "    recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    print \"the recall for this model is :\", recall\n",
    "    print \"The accuracy is :\", (cm[1,1]+cm[0,0])/(cm[0,0] + cm[0,1] + cm[1,0] + cm[1,1])\n",
    "    loss = (1 - precision) * 88.29 + (1 - recall) * 122.12\n",
    "    fig= plt.figure(figsize=(6,3))# to plot the graph\n",
    "    print \"TP\",cm[1,1] # no of fraud transaction which are predicted fraud\n",
    "    print \"TN\",cm[0,0] # no. of normal transaction which are predited normal\n",
    "    print \"FP\",cm[0,1] # no of normal transaction which are predicted fraud\n",
    "    print \"FN\",cm[1,0] # no of fraud Transaction which are predicted normal\n",
    "    sns.heatmap(cm, cmap=\"coolwarm_r\", annot=True, linewidths=0.5)\n",
    "    plt.title(\"Confusion_matrix\")\n",
    "    plt.xlabel(\"Predicted_class\")\n",
    "    plt.ylabel(\"Real class\")\n",
    "    plt.show()\n",
    "    print \"Classification Report:\" \n",
    "    print(classification_report(labels_test,pred))\n",
    "    print \"The loss is : \", loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_fuction(model,features_train,features_test,labels_train,labels_test):\n",
    "    model.fit(features_train,labels_train.values.ravel())\n",
    "    pred = model.predict(features_test)\n",
    "    cm = confusion_matrix(labels_test,pred)\n",
    "    recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    loss = (1 - precision) * 88.29 + (1 - recall) * 122.12\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters using custom grid-search like methods with 5-fold cv\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "#### C, penalty\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.992481203008\n",
      "the fraud transacation proportion is : 0.00751879699248\n",
      "total number of record in resampled data is: 65436\n"
     ]
    }
   ],
   "source": [
    "undersample_data_lr = undersample(df, normal_indices,fraud_indices, 132)\n",
    "X_undersample_lr = undersample_data_lr.iloc[:, undersample_data_lr.columns != \"Class\"]\n",
    "y_undersample_lr = undersample_data_lr.iloc[:, undersample_data_lr.columns == \"Class\"]\n",
    "X_undersample_train_lr, X_undersample_test_lr, y_undersample_train_lr, y_undersample_test_lr = train_test_split(X_undersample_lr, y_undersample_lr, random_state=0)\n",
    "X_lr = df.iloc[:, df.columns != \"Class\"]\n",
    "y_lr = df.iloc[:, df.columns == \"Class\"]\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_lr(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    c_param_range = [0.01, 0.1, 1, 10, 100]\n",
    "    penalties = ['l1', 'l2']\n",
    "\n",
    "    penalty_list = []\n",
    "    c_list = []\n",
    "    mean_loss_list = []\n",
    "    \n",
    "    j = 0\n",
    "    for penalty in penalties:\n",
    "        print '-------------------------------------------'\n",
    "        print 'Penalty: ', penalty \n",
    "        print '-------------------------------------------'\n",
    "        for c_param in c_param_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'C parameter: ', c_param\n",
    "            print '-------------------------------------------'\n",
    "            print ''\n",
    "\n",
    "            loss_list = []\n",
    "            for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                # Call the logistic regression model with a certain C parameter\n",
    "                lr = LogisticRegression(C=c_param, penalty=penalty, random_state=0)\n",
    "\n",
    "                # Calculate the custom loss and append it to a list for loss representing the current c_parameter\n",
    "                loss = custom_loss_fuction(lr, X_train_data.iloc[train], X_test_lr, y_train_data.iloc[train], y_test_lr)\n",
    "                loss_list.append(loss)\n",
    "                print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "            j += 1\n",
    "            print ''\n",
    "            print 'Mean loss', np.mean(loss_list)\n",
    "            print ''\n",
    "            penalty_list.append(penalty)\n",
    "            c_list.append(c_param)\n",
    "            mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Penalty', 'C_parameter','Mean loss'])\n",
    "    results_table['Penalty'] = penalty_list\n",
    "    results_table['C_parameter'] = c_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_penalty = results_table.loc[results_table['Mean loss'].idxmin()]['Penalty']\n",
    "    best_c = results_table.loc[results_table['Mean loss'].idxmin()]['C_parameter']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '************************************************************************************'\n",
    "    print 'Best model to choose from cross validation is with Penalty = ', best_penalty, 'and best c = ', best_c\n",
    "    print '************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Penalty:  l1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  50.24541304347825\n",
      "Fold  2 : loss =  50.24541304347825\n",
      "Fold  3 : loss =  56.98232258064516\n",
      "Fold  4 : loss =  51.37908424908426\n",
      "Fold  5 : loss =  54.513999999999996\n",
      "\n",
      "Mean loss 52.67324658333719\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  40.88484745762711\n",
      "Fold  2 : loss =  40.79644927536233\n",
      "Fold  3 : loss =  44.90254277286135\n",
      "Fold  4 : loss =  43.103548672566376\n",
      "Fold  5 : loss =  41.948807017543864\n",
      "\n",
      "Mean loss 42.32723903919221\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  44.416621468926564\n",
      "Fold  2 : loss =  42.04320512820514\n",
      "Fold  3 : loss =  41.948807017543864\n",
      "Fold  4 : loss =  41.30455457227139\n",
      "Fold  5 : loss =  43.81548717948718\n",
      "\n",
      "Mean loss 42.705735073286824\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  45.007652661064434\n",
      "Fold  2 : loss =  42.65073446327685\n",
      "Fold  3 : loss =  41.948807017543864\n",
      "Fold  4 : loss =  41.30455457227139\n",
      "Fold  5 : loss =  43.203988505747134\n",
      "\n",
      "Mean loss 42.82314744398074\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  45.007652661064434\n",
      "Fold  2 : loss =  42.65073446327685\n",
      "Fold  3 : loss =  41.948807017543864\n",
      "Fold  4 : loss =  41.30455457227139\n",
      "Fold  5 : loss =  43.203988505747134\n",
      "\n",
      "Mean loss 42.82314744398074\n",
      "\n",
      "-------------------------------------------\n",
      "Penalty:  l2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  58.0780701754386\n",
      "Fold  2 : loss =  60.30960451977401\n",
      "Fold  3 : loss =  61.538307692307704\n",
      "Fold  4 : loss =  56.28592982456141\n",
      "Fold  5 : loss =  93.11684488448844\n",
      "\n",
      "Mean loss 65.86575141931402\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.324848739495806\n",
      "Fold  2 : loss =  60.37716666666667\n",
      "Fold  3 : loss =  60.13909641873279\n",
      "Fold  4 : loss =  55.03366145833334\n",
      "Fold  5 : loss =  93.11684488448844\n",
      "\n",
      "Mean loss 65.19832363354342\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.324848739495806\n",
      "Fold  2 : loss =  59.17700709219857\n",
      "Fold  3 : loss =  60.65345355191258\n",
      "Fold  4 : loss =  57.393385964912284\n",
      "Fold  5 : loss =  93.11684488448844\n",
      "\n",
      "Mean loss 65.53310804660154\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.324848739495806\n",
      "Fold  2 : loss =  59.17700709219857\n",
      "Fold  3 : loss =  60.65345355191258\n",
      "Fold  4 : loss =  57.393385964912284\n",
      "Fold  5 : loss =  93.11684488448844\n",
      "\n",
      "Mean loss 65.53310804660154\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  57.324848739495806\n",
      "Fold  2 : loss =  59.17700709219857\n",
      "Fold  3 : loss =  60.65345355191258\n",
      "Fold  4 : loss =  58.28833333333334\n",
      "Fold  5 : loss =  93.11684488448844\n",
      "\n",
      "Mean loss 65.71209752028575\n",
      "\n",
      "  Penalty  C_parameter  Mean loss\n",
      "0      l1         0.01  52.673247\n",
      "1      l1         0.10  42.327239\n",
      "2      l1         1.00  42.705735\n",
      "3      l1        10.00  42.823147\n",
      "4      l1       100.00  42.823147\n",
      "5      l2         0.01  65.865751\n",
      "6      l2         0.10  65.198324\n",
      "7      l2         1.00  65.533108\n",
      "8      l2        10.00  65.533108\n",
      "9      l2       100.00  65.712098\n",
      "\n",
      "************************************************************************************\n",
      "Best model to choose from cross validation is with Penalty =  l1 and best c =  0.1\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_lr = Kfold_tuning_lr(X_undersample_train_lr, y_undersample_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Logistic Regression Model\n",
    "* C=0.1, penalty=l1\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 132 proportion\n",
      "the recall for this model is : 0.7916666666666666\n",
      "The accuracy is : 0.9993960843796522\n",
      "TP 95\n",
      "TN 71064\n",
      "FP 18\n",
      "FN 25\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAADhCAYAAADCg66ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADx0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wcmMxLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvjNbHMQAAHlZJREFUeJzt3XucVVX9//HXe2YQEEQQ8QbeRUktScr8esssCVOTvr8szVtqUV7KtDL1a3lLsyyvmYmK9y+kfjNJVMRKyUrFu0JeCDFAFAQBUa4zn98few0dx5lzDszlzNm8n4/Hfnj22muv/TnT6XMWa6+ztiICMzPLn5pKB2BmZu3DCd7MLKec4M3McsoJ3swsp5zgzcxyygnezCynnODNzHLKCd7WmKTukv4oaaGkO1vRzhGSHmzL2CpN0haSFkuqrXQstvZygl9LSPqapCdT0pkt6X5Je7Wy2S8DGwN9I+LQNW0kIm6PiKGtjKXDSJou6XPF6kTEvyOiZ0TUd1RcZk05wa8FJJ0GXA5cRJaQtwB+AxzSyqa3BF6JiJWtbCdXJNVVOgYzACLCW443YH1gMXBoC8e7kiX/N9J2OdA1HdsXmAl8H5gDzAaOTcfOA5YDK1L7xwPnArcVtL0VEEBd2v86MA14F3gNOKKg/NGC8/YAJgEL03/3KDj2MHAB8LfUzoPAhiX+Bo1xHAvMAN4Bvg18EngeWAD8uqD+tsCfgXnA28DtQO907FagAViS3vfpBe0fD/wbmFj43oEN0t/x4NRGT2AqcHSlPx/e8r1VPABv7fw/MAwDVjYm2WaOnw88BmwE9AP+DlyQju2bzj0f6AJ8AXgf6JOON03oLSZ4oAewCNghHdsU2Cm9XpXgUzJ8BzgqnXd42u+bjj8M/AvYHuie9i8u8TdojOO3QDdgKLAU+EN63/3JvsA+nepvB+xP9uXXLyXsywvamw58rpn2b0nvszsf/nIbCryZrncdcFelPxve8r95iCb/+gJvR8vDKEcA50fEnIiYS9YzP6rg+Ip0fEVE3EfWa91hDWNpAHaW1D0iZkfE5GbqHAi8GhG3RsTKiBgNvAQcXFDnxoh4JSKWAHcAg8u8/gURsTQiHgTeA0an9z0L+CvwcYCImBoREyJiWfqbXAp8uoz2z42I91JcH5CueSfwJ7Ivym+VGbPZGnOCz795wIZFxoU3A14v2H89la06v8mXw/tkQwyrJSLeA75KNjQyW9I4SYPKiKcxpv4F+2+uYTxvFbxe0sx+TwBJG0saI2mWpEXAbcCGZbQ/o8TxkcDOwE0RMa/MmM3WmBN8/v0DWAYMb+H4G2Q3SxttkcrWxHvAugX7mxQejIjxEbE/2fDMS2RDFaXiaYxp1hrGtCYuIhte+WhE9AKOBFRwvKU1tltceztNlxxJNoxzoqTt2ihWsxY5wedcRCwEfgJcLWm4pHUldZF0gKRfAKOBsyX1k7RhqnvbGl7uWWCfNAd8feDMxgOpV3yIpB5kXziLyYZsmroP2D5N66yT9FVgR+DeNYxpTayX4lsoqT/wwybH3wK2Wc02zyL7AjgOuAS4xXPkrb05wa8FIuJXwGnA2cBcsqGEk8luMv4UeJJsNskLwNOpbE2uMwH4XWrrKT6YlGtSDG8A88nGtE9opo15wEFkM3fmkc1SOSgi3l6TmNbQecCuZLN4xgG/b3L8Z2Rfigsk/aBUY5KGkL33oyObF/9zsmR/RptGbdaEIvxEJzOzPHIP3swsp5zgLRfSejaLm9mam4pptlbwEI2ZWU65B29mllOdeVEk/9PCzMql0lWK2+vgR4rmnEf/+OlWX6OjdeYEz14HP1LpEKwTefSP2WoB47qs6UoJlkcHrni5TdpRTf4GNDp1gjcz6yg1tfn73ZkTvJkZUFPnBG9mlks1NVU3xF6SE7yZGR6iMTPLLd9kNTPLqVr34M3M8kkegzczyyePwZuZ5ZR78GZmOeUxeDOznKqp8ywaM7NcqpETvJlZLrkHb2aWU5JvspqZ5VKtFxszM8sn9+DNzHKq1mPwZmb5VFPrBG9mlks1HqIxM8unPE6TzN87MjNbA5KKbmW20VvSXZJekvRPSf8laQNJEyS9mv7bJ9WVpCslTZX0vKRdC9o5JtV/VdIxBeVDJL2QzrlSJQJzgjczA2pra4puZboCeCAiBgG7AP8EzgD+FBEDgT+lfYADgIFpGwFcAyBpA+Ac4FPAbsA5jV8Kqc43C84bViwYJ3gzM7LVJIttJc+X1gf2AW4AiIjlEbEAOAS4OVW7GRieXh8C3BKZx4DekjYFPg9MiIj5EfEOMAEYlo71iojHIiKAWwraapYTvJkZUFuropukEZKeLNhGNGlia2AucKOkZyRdL6kHsHFEzE513gQ2Tq/7AzMKzp+ZyoqVz2ymvEW+yWpmBiWHYSJiJDCySJU6YFfgOxHxuKQr+M9wTGMbISlaG2u53IM3M6P1QzRkPeqZEfF42r+LLOG/lYZXSP+dk47PAjYvOH9AKitWPqCZ8hY5wZuZUXqIppSIeBOYIWmHVPRZYAowFmicCXMMcE96PRY4Os2m2R1YmIZyxgNDJfVJN1eHAuPTsUWSdk+zZ44uaKtZHqIxM6PN1qL5DnC7pHWAacCxZB3pOyQdD7wOfCXVvQ/4AjAVeD/VJSLmS7oAmJTqnR8R89PrE4GbgO7A/WlrkRO8mRmU1UsvJSKeBT7RzKHPNlM3gJNaaGcUMKqZ8ieBncuNxwnezAyvJmlmlltt0YPvbJzgzcxwgjczyy0P0ZiZ5ZR78FbS5v27c/7pO67a32yTblx/+3TuHPuf3yNsMaA7Z50yiO237cl1t77G6LtnNtfUaulSJ84+bRA7bLsei95dwU9+MYU35yxbdXzjfl259epPcuPo6W1yPetYH7vuIjb6wr4snzOPiR8/GIBeuwxi56vPo6ZbV2JlPS9+51wWTnqhwpFWL+XwV0E5fEuVNWPWEo495SmOPeUpjj/1KZYua2DiP97+QJ1F767k8pFTGXP3jBZaadkmG3Xlqot2+VD5QUM35d3FKznsW0/wu3tmcsLXt/nA8ZOP35bHn5r/ofOsOsy8+fc8cdA3PlA26Gc/5NULrubRTwznlXOv4CM/+2GFosuH2hoV3apRu/XgJQ0iWy2tcTGcWcDYiPhne12zsxmySx9mzV7CW3OXfaB8wcIVLFi4gj0+scGHzhm670Z8+eD+dKmrYcori/jVNa/S0FD6Wnt9qi+j/vd1AB7+21xO/fbAVcf23r0vs99aytKl9a17Q1Yx8x99ku5bNllXKoK6Xj0A6LL+eix9Y04zZ1q5cjgE3z49eEk/AsYAAp5Im4DRks4odm6efG7vfjw0sfz/0205YF0+u/dGnHD6sxx7ylM0NARDP71x6ROBfn27MuftpQDUN8B7761k/V51dO9WwxH/bwtuHD19Dd6BdWZTvn8RH7n4dPab9jAf+fmPePnsSysdUlVr7VIFnVF79eCPB3aKiBWFhZIuBSYDF7fTdTuNujqx56c25Le3vFb2OUN26c0O2/bk+kuzB7t0XaeGdxZkf8KLztqJTTfuRl2d2LhfN268YggAd46dyX1/eqvFNo/72lbccc9Mliwt458BVlW2+NbhTPnBz3jz7gfZ9MsH8LGRF/L4sGMrHVbVqtYkXkx7JfgGYDOydRcKbZqONSutrzwC4NprrwV2aKlqp7f7kA145V/vrkrQ5ZDg/j+/xbXNfCmcddFkIBuD/5/vDeI7Zz33geNz5y1jow27MXfecmproEePOhYuWsmO2/di3z36ccLXt6FnjzoigmXLG/j9uDda9wat4gYc9SWmnHohALPvup+PXvvTCkdU3fI4RNNeCf57wJ8kvcp/Fq7fAtgOOLmlk5qstxy3/PGRdgqv/X1un4146JHVGxN96rkF/OzsnfjdPTNZsHAF6/WsY93utR8aw2/O3x6fxwGf3ZjJLy9i3z378fTz7wBw0hnPrqpz3OFbsmRpvZN7Tix7Yw4b7LMb8yc+Qd/P7M77U6dXOqSqVv5T+apHuyT4iHhA0vZkzxMsvMk6KSJyf6evW9caPjm4D5dc/cqqskOGbQrAPQ/MZoPeXbj+siH0WLeWhgY49IsDOPLESUyf8T7X3Tqdy87/GBLU1weX/vbVshL8vRNm8+PTPsKYa3dj0eIVnPuLteZe9lph8K2/ou+nd2OdDfuw32uP8Or5V/H8CT9mp0vPQnV11C9dxvMn/KTSYVa1mhwmeGULmnVKsdfB1duDt7b36B8/DcC4LtU7dGdt78AVL0M2iaNVrr6fosnwpANaf42O5h86mZkBVTrVvSgneDMz8jlE4wRvZgbU1lY6grbnBG9mhodozMxyy0M0ZmY55R68mVlO1dSUmjJefd8ATvBmZrgHb2aWWx6DNzPLqVoP0ZiZ5VMeV5PM4T9KzMxWX62i6FYOSbWSnpF0b9q/SdJrkp5N2+BULklXSpoq6XlJuxa0cYykV9N2TEH5EEkvpHOulEp/JTnBm5mR9eCLbWU6BWi6lOsPI2Jw2hrX7z4AGJi2EcA1WQzaADgH+BTZarznSOqTzrkG+GbBecNKBVMywUvaU1KP9PpISZdK2rLUeWZm1aS2JopupUgaABwIXF/G5Q4BbonMY0BvSZsCnwcmRMT8iHgHmAAMS8d6RcRjkS0BfAswvNRFyunBXwO8L2kX4PvAv1LjZma50QY9+MuB0/nwU+suTMMwl0nqmsr685+HIQHMTGXFymc2U15UOQl+ZfrGOAT4dURcDaxXxnlmZlWj1Bi8pBGSnizYRjSeK+kgYE5EPNWk2TOBQcAngQ2AH3XcOypvFs27ks4EjgT2kVQDdGnfsMzMOlapYZgmjxRtak/gi5K+AHQDekm6LSKOTMeXSboR+EHanwVsXnD+gFQ2C9i3SfnDqXxAM/WLKqcH/1VgGXB8RLyZGr6kjPPMzKqGiKJbMRFxZkQMiIitgMOAP0fEkWnsnDTjZTjwYjplLHB0mk2zO7AwImYD44Ghkvqkm6tDgfHp2CJJu6e2jgbuKfWeyurBA1dERH16zuogYHQZ55mZVY3Sa9Gskdsl9SP7ldSzwLdT+X3AF4CpwPvAsQARMV/SBcCkVO/8iJifXp8I3AR0B+5PW1HlJPiJwN7p2+TBdOGvAkeUca6ZWVWoKdFLL1dEPEw2rEJE7NdCnQBOauHYKGBUM+VPAjuvTizlDNEoIt4H/hv4TUQcuroXMTPr7GpqouhWjcpK8JL+i6zHPm41zjMzqxqtGYPvrMoZojmFbKrP3RExWdI2wF/aNywzs45V7nIE1aRkgo+IiWTj8I3704DvtmdQZmYdrUZNf59U/Uom+HQH+HRgJ7L5nUDLNw/MzKqRctiDL2cs/XbgJWBr4DxgOv+ZwmNmlgttsZpkZ1NOgu8bETcAKyLikYg4DnDv3cxyZW29yboi/Xe2pAOBN8jWVDAzy421cgwe+Kmk9clWkrwK6AWc2q5RmZl1sDyOwZczi+be9HIh8Jn2DcfMrDJqP7TKb/VrMcFLugpaHniKCE+VNLPcWNuGaJ7ssCjMzCqsWm+kFtNigo+ImzsyEDOzSspjD76cZ7JOkNS7YL+PpPHtG5aZWcdaW6dJ9ouIBY07EfGOpI3aMSYzsw5Xk8ObrOX80Kle0haNO5K2pMjNVzOzapTHHryydeeLVJCGkT2H8BGyp5LsDYyIiPYepqnOv6iZVYJa28C/pk0rmnO23WabVl+jo5UzD/4BSbsCu6ei70XE2+0bVmZclx064jJWJQ5c8TLgz4V9UOPnorVqIn9DNOWMwZMS+r0lK5qZVSmtrQnezCzvaqK+0iG0OSd4MzPWsh86SSq6YmREzG/7cMzMKmNt68E/RTaTpbk7xwFs0y4RmZlVgErMKKxGxZYq2LojAzEzq6S1rQe/iqQ+wEA++EzWiS2fYWZWXWoa8pfgy1mL5hvARGA82TNZxwPntm9YZmYdSzQU3UqeL3WT9ISk5yRNlnReKt9a0uOSpkr6naR1UnnXtD81Hd+qoK0zU/nLkj5fUD4slU2VdEapmMpZquAU4JPA6xHxGeDjwILip5iZVRc11BfdyrAM2C8idgEGA8Mk7Q78HLgsIrYD3gGOT/WPB95J5ZelekjaETgM2AkYBvxGUq2kWuBq4ABgR+DwVLdF5ST4pRGxNF24a0S8BPinhGaWK61diyYyi9Nul7QFsB9wVyq/GRieXh+S9knHPytJqXxMRCyLiNeAqcBuaZsaEdMiYjkwJtVtUTlj8DPTcsF/ACZIegd4vYzzzMyqRpm99OJtZL3sp4DtyHrb/wIWRMTKVGUm0D+97g/MAIiIlZIWAn1T+WMFzRaeM6NJ+aeKxVPOWjRfSi/PlfQXYH3ggVLnmZlVk1JLFUgaAYwoKBoZESML60REPTA4dYrvBga1dZyro9xZNHsBAyPiRkn9yL5NXmvXyMzMOlCpHnxK5iOLVvpP3QWpQ/xfQG9JdakXPwCYlarNAjYnGyWpI+s8zysob1R4TkvlzSpnFs05wI+AM1NRF+C2UueZmVUTRUPRreT5Ur/Gp99J6g7sD/wT+Avw5VTtGOCe9Hps2icd/3Nk67ePBQ5Ls2y2Jpui/gQwCRiYZuWsQ3YjdmyxmMrpwX+JbObM0wAR8Yak9co4z8ysaqj1P3TaFLg5jcPXAHdExL2SpgBjJP0UeAa4IdW/AbhV0lRgPlnCJiImS7oDmAKsBE5KQz9IOplsqnotMCoiJhcLqJwEvzwiQlKkC/RYrbdsZlYFWnuTNSKeJ+sMNy2fRjYDpmn5UuDQFtq6ELiwmfL7gPvKjamcaZJ3SLqWbBzpm8BDwPXlXsDMrCpEFN+qUDmzaH4paX9gEdn8959ExIR2j8zMrAO1xTTJzqbcJzpNACYASKqRdERE3N6ukZmZdaA8PtGpxSEaSb3Segi/ljRUmZOBacBXOi5EM7P21wZLFXQ6xXrwt5Ktm/AP4BvAWWRrww+PiGc7IDYzs47TkL8efLEEv01EfBRA0vXAbGCLxnVpzMxypUp76cUUS/ArGl9ERL2kmU7uZpZX1ToMU0yxBL+LpEXptYDuaV9kC6f1avfozMw6Sg5vshZ7ZF9tRwZiZlZJql+7evBmZmuPKv0xUzFO8GZmsNbdZDUzW3usZdMkzczWHu7Bm5nllG+ympnl1No0TdLMbK3iHryZWU75JquZWU75JquZWU41+IdO1s66DdiEwTf+gnU26gsR/PuGO5h+1S0M/PHJbHH8V1j29nwAXj77UuY+MLHC0VpH2eo7R7PFcYeCxL9H3cn0K2/2Z6KNhcfgrb3FynqmnH4xi56ZQm3PHuz1+P/x9kN/A+C1K25i2mWjKhyhdbSeOw1ki+MO5dE9DiWWr2C3cdczZ9xfAH8m2lLUr6x0CG3OCb6TWfbmXJa9OReA+sXvsfilaXTbbOMKR2WV1HPQtiyY9DwNS7LVuudNnMQmw4dWOKocyuFaNC0+sq+9SDq2o69Zrbpv2Z/1B3+EBU88B8CWJx7B3k+P5WPXXURdb6/WvLZYPPkV+uw5hC4b9Kamezc2OmAfum++CeDPRJuqry++VaEOT/DAeS0dkDRC0pOSnhw5cmRHxtTp1PZYlyF3XMmU71/Eynff4/VrR/OXHfbnr0MOYdnsOex4yRmVDtE6yOKXpjHtl9fzqftvYLdx17PouZeI+gZ/JtpYNDQU3apRuwzRSHq+pUNAi+MNETESaMzsMe6kX7V1aFVBdXUMueNKZo3+I2/+YQIAy+fMW3X83zfcySf/8NtKhWcVMOPGu5hx410A7HDBqSyd9ZY/E23MN1nLtzHwebKHdhcS8Pd2umZufOy6C1n80jReu/ymVWVdN+m3amx+k+Gf493Jr1YoOquEdfptwPK58+m2+aZsMnwof9vrK/5MtDVPkyzbvUDPiHi26QFJD7fTNXOhz55DGHDkcBa98DJ7PfkHIJv+ttlhB9Frl0EQsGT6LF448ScVjtQ60pA7rqLLBr2JlSt58bvnsXLhu+x0xY/9mWhDre3BSxoFHATMiYidU9m5wDeBuanaWRFxXzp2JnA8UA98NyLGp/JhwBVALXB9RFycyrcGxgB9gaeAoyJiedGYovPeOY5xXXaodAzWiRy44mUA/LmwQulzoda2s+jy04omw17fu7ToNSTtAywGbmmS4BdHxC+b1N0RGA3sBmwGPARsnw6/AuwPzAQmAYdHxBRJdwC/j4gxkn4LPBcR1xSLqRI3Wc3MOp+GhuJbCRExEZhf5tUOAcZExLKIeA2YSpbsdwOmRsS01DsfAxwiScB+wF3p/JuB4aUu4gRvZkY2RFNsa4WTJT0vaZSkPqmsPzCjoM7MVNZSeV9gQUSsbFJelBO8mRkQDVF0K5zGnbYRZTR7DbAtMBiYDXTo1ED/ktXMjGyZkKLHPziNu7w2I95qfC3pOrIJKACzgM0Lqg5IZbRQPg/oLaku9eIL67fIPXgzMyCioei2JiRtWrD7JeDF9HoscJikrml2zEDgCbKbqgMlbS1pHeAwYGxks2H+Anw5nX8McE+p67sHb2ZG6R58KZJGA/sCG0qaCZwD7CtpMBDAdOBbABExOc2KmQKsBE6KiPrUzsnAeLJpkqMiYnK6xI+AMZJ+CjwD3FAqJid4MzOgoZUJPiIOb6a4xSQcERcCFzZTfh9wXzPl08hm2ZTNCd7MDOjEvwlaY07wZma0foimM3KCNzMjmyaZN07wZma0fgy+M3KCNzODql3zvRgneDMzIOqd4M3McslDNGZmOeUhGjOznGpY6QRvZpZL7sGbmeVU/QoneDOzXHIP3swspzwGb2aWU54maWaWU16Lxswsp3yT1cwsp3yT1cwsp9yDNzPLKY/Bm5nlVMMKz6IxM8slD9GYmeVUQ72HaMzMcslDNGZmOeUevJlZTtUv8xi8mVkuxQr34M3Mcql+iXvwZma5VL/EN1k71IErXq50CNYJ+XNh7aFhZf6GaBSRvzeVN5JGRMTISsdhnYs/F1ZKTaUDsLKMqHQA1in5c2FFOcGbmeWUE7yZWU45wVcHj7Nac/y5sKJ8k9XMLKfcgzczyykn+E5O0jBJL0uaKumMSsdjlSdplKQ5kl6sdCzWuTnBd2KSaoGrgQOAHYHDJe1Y2aisE7gJGFbpIKzzc4Lv3HYDpkbEtIhYDowBDqlwTFZhETERmF/pOKzzc4Lv3PoDMwr2Z6YyM7OSnODNzHLKCb5zmwVsXrA/IJWZmZXkBN+5TQIGStpa0jrAYcDYCsdkZlXCCb4Ti4iVwMnAeOCfwB0RMbmyUVmlSRoN/APYQdJMScdXOibrnPxLVjOznHIP3swsp5zgzcxyygnezCynnODNzHLKCd7MLKec4M3McsoJ3polqV7Ss5JelHSnpHVb0da+ku5Nr79YbNljSb0lnbgG1zhX0g9W85ytvOSu5ZkTvLVkSUQMjoidgeXAtwsPKrPan5+IGBsRFxep0htY7QRvZh/mBG/l+CuwXerxvizpFuBFYHNJQyX9Q9LTqaffE1Y9qOQlSU8D/93YkKSvS/p1er2xpLslPZe2PYCLgW3Tvx4uSfV+KGmSpOclnVfQ1v9IekXSo8AOxd6ApO0kPZSu87SkbZsc30rSX9Oxp1MsSNpU0sSCf83sLalW0k1p/wVJp7bB39iszdVVOgDr3CTVkT1w5IFUNBA4JiIek7QhcDbwuYh4T9KPgNMk/QK4DtgPmAr8roXmrwQeiYgvpYeb9ATOAHaOiMHp+kPTNXcDBIyVtA/wHtnaPIPJPsdPA08VeSu3AxdHxN2SupF1bjYqOD4H2D8ilkoaCIwGPgF8DRgfERemGNdN1+yf/nWDpN4l/oxmFeEEby3pLunZ9PqvwA3AZsDrEfFYKt+d7ElTf5MEsA7ZGimDgNci4lUASbcBI5q5xn7A0QARUQ8slNSnSZ2haXsm7fckS/jrAXdHxPvpGi0uwiZpPbKEfHe61tJUXlitC/BrSYOBemD7VD4JGCWpC/CHiHhW0jRgG0lXAeOAB1u6tlklOcFbS5Y09qIbpYT4XmERMCEiDm9S7wPntZKAn0XEtU2u8b02vAbAqcBbwC5kvfulkD09Kf2L4UDgJkmXRsQtknYBPk92b+IrwHFtHI9Zq3kM3lrjMWBPSdsBSOohaXvgJWCrgnHuw1s4/0/ACencWknrA++S9c4bjQeOKxjb7y9pI2AiMFxS99RDP7ilICPiXWCmpOGpja7NzApaH5gdEQ3AUUBtqrsl8FZEXAdcD+yahqZqIuL/yIaodi3+ZzKrDCd4W2MRMRf4OjBa0vOk4Zk0BDICGJduss5poYlTgM9IeoFs/HzHiJhHNuTzoqRLIuJB4H+Bf6R6dwHrRcTTZGP7zwH3kw2lFHMU8N0U59+BTZoc/w1wjKTnyIaYGv+lsi/wnKRngK8CV5A9NvHhNIR1G3BmiWubVYSXCzYzyyn34M3Mcso3WS1XJF0N7Nmk+IqIuLES8ZhVkodozMxyykM0ZmY55QRvZpZTTvBmZjnlBG9mllNO8GZmOfX/AbZNse2160QhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71082\n",
      "          1       0.84      0.79      0.82       120\n",
      "\n",
      "avg / total       1.00      1.00      1.00     71202\n",
      "\n",
      "The loss is :  39.50556047197641\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 132 proportion\"\n",
    "optimal_lr = LogisticRegression(C=0.1, penalty='l1', random_state=0)\n",
    "prediction_algorithms(optimal_lr, X_undersample_train_lr, X_test_lr, y_undersample_train_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (kernel='rbf')\n",
    "#### C, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.961538461538\n",
      "the fraud transacation proportion is : 0.0384615384615\n",
      "total number of record in resampled data is: 12792\n"
     ]
    }
   ],
   "source": [
    "undersample_data_svm = undersample(df_fe, normal_indices,fraud_indices, 25)\n",
    "X_undersample_svm = undersample_data_svm.iloc[:, undersample_data_svm.columns != \"Class\"]\n",
    "y_undersample_svm = undersample_data_svm.iloc[:, undersample_data_svm.columns == \"Class\"]\n",
    "X_undersample_train_svm, X_undersample_test_svm, y_undersample_train_svm, y_undersample_test_svm = train_test_split(X_undersample_svm, y_undersample_svm, random_state=0)\n",
    "X_svm = df_fe.iloc[:, df_fe.columns != \"Class\"]\n",
    "y_svm = df_fe.iloc[:, df_fe.columns == \"Class\"]\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm, y_svm, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_svm(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    c_param_range = [0.1, 1, 2, 5]\n",
    "    gamma_range = [0.01, 0.1, 'auto', 1]\n",
    "\n",
    "    c_list = []\n",
    "    gamma_list = []\n",
    "    mean_loss_list = []\n",
    "    \n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'C parameter: ', c_param\n",
    "        print '-------------------------------------------'\n",
    "        for gamma in gamma_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'Gamma: ', gamma\n",
    "            print '-------------------------------------------'\n",
    "            print ''\n",
    "\n",
    "            loss_list = []\n",
    "            for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                # Call the logistic regression model with a certain C parameter\n",
    "                svm = SVC(C=c_param, gamma=gamma, random_state=0)\n",
    "\n",
    "                # Calculate the custom loss and append it to a list for loss representing the current c_parameter\n",
    "                loss = custom_loss_fuction(svm, X_train_data.iloc[train], X_test_svm, y_train_data.iloc[train], y_test_svm)\n",
    "                loss_list.append(loss)\n",
    "                print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "            j += 1\n",
    "            print ''\n",
    "            print 'Mean loss', np.mean(loss_list)\n",
    "            print ''\n",
    "            gamma_list.append(gamma)\n",
    "            c_list.append(c_param)\n",
    "            mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['C_parameter','Gamma', 'Mean loss'])\n",
    "    results_table['Gamma'] = gamma_list\n",
    "    results_table['C_parameter'] = c_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_gamma = results_table.loc[results_table['Mean loss'].idxmin()]['Gamma']\n",
    "    best_c = results_table.loc[results_table['Mean loss'].idxmin()]['C_parameter']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '************************************************************************************'\n",
    "    print 'Best model to choose from cross validation is with C = ', best_c, 'and best gamma = ', best_gamma\n",
    "    print '************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcus/venv0/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  nan\n",
      "Fold  2 : loss =  nan\n",
      "Fold  3 : loss =  nan\n",
      "Fold  4 : loss =  nan\n",
      "Fold  5 : loss =  nan\n",
      "\n",
      "Mean loss nan\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  56.989333333333335\n",
      "Fold  2 : loss =  43.75966666666666\n",
      "Fold  3 : loss =  46.812666666666665\n",
      "Fold  4 : loss =  48.848000000000006\n",
      "Fold  5 : loss =  42.742\n",
      "\n",
      "Mean loss 47.830333333333336\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  53.93633333333333\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  46.812666666666665\n",
      "Fold  5 : loss =  38.67133333333333\n",
      "\n",
      "Mean loss 45.1844\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  73.44226356589147\n",
      "Fold  2 : loss =  61.47018181818182\n",
      "Fold  3 : loss =  63.379666666666665\n",
      "Fold  4 : loss =  70.41494389438944\n",
      "Fold  5 : loss =  55.280343234323425\n",
      "\n",
      "Mean loss 64.79747983589056\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Gamma:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  74.50066666666667\n",
      "Fold  2 : loss =  64.8145\n",
      "Fold  3 : loss =  66.68110897435896\n",
      "Fold  4 : loss =  70.41432692307693\n",
      "Fold  5 : loss =  54.09984313725491\n",
      "\n",
      "Mean loss 66.1020891402715\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  auto\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "-------------------------------------------\n",
      "Gamma:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Fold  1 : loss =  52.918666666666674\n",
      "Fold  2 : loss =  42.742\n",
      "Fold  3 : loss =  43.75966666666666\n",
      "Fold  4 : loss =  45.795\n",
      "Fold  5 : loss =  37.65366666666667\n",
      "\n",
      "Mean loss 44.5738\n",
      "\n",
      "    C_parameter Gamma  Mean loss\n",
      "0           0.1  0.01        NaN\n",
      "1           0.1   0.1        NaN\n",
      "2           0.1  auto        NaN\n",
      "3           0.1     1        NaN\n",
      "4           1.0  0.01  47.830333\n",
      "5           1.0   0.1  44.573800\n",
      "6           1.0  auto  45.184400\n",
      "7           1.0     1  44.573800\n",
      "8           2.0  0.01  64.797480\n",
      "9           2.0   0.1  44.573800\n",
      "10          2.0  auto  44.573800\n",
      "11          2.0     1  44.573800\n",
      "12          5.0  0.01  66.102089\n",
      "13          5.0   0.1  44.573800\n",
      "14          5.0  auto  44.573800\n",
      "15          5.0     1  44.573800\n",
      "\n",
      "************************************************************************************\n",
      "Best model to choose from cross validation is with C =  1.0 and best gamma =  0.1\n",
      "************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_svm = Kfold_tuning_svm(X_undersample_train_svm, y_undersample_train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 25 proportion\n",
      "the recall for this model is : 0.7916666666666666\n",
      "The accuracy is : 0.9996488862672397\n",
      "TP 95\n",
      "TN 71082\n",
      "FP 0\n",
      "FN 25\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71082\n",
      "          1       1.00      0.79      0.88       120\n",
      "\n",
      "avg / total       1.00      1.00      1.00     71202\n",
      "\n",
      "The loss is :  25.441666666666674\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 25 proportion\"\n",
    "optimal_svm = SVC(gamma=0.1, random_state=0)\n",
    "prediction_algorithms(optimal_svm, X_undersample_train_svm, X_test_svm, y_undersample_train_svm, y_test_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### n_estimators, max_features, max_depth\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.975609756098\n",
      "the fraud transacation proportion is : 0.0243902439024\n",
      "total number of record in resampled data is: 20172\n"
     ]
    }
   ],
   "source": [
    "undersample_data_rf = undersample(df_fe, normal_indices,fraud_indices, 40)\n",
    "X_undersample_rf = undersample_data_rf.iloc[:, undersample_data_rf.columns != \"Class\"]\n",
    "y_undersample_rf = undersample_data_rf.iloc[:, undersample_data_rf.columns == \"Class\"]\n",
    "X_undersample_train_rf, X_undersample_test_rf, y_undersample_train_rf, y_undersample_test_rf = train_test_split(X_undersample_rf, y_undersample_rf, random_state=0)\n",
    "X_rf = df_fe.iloc[:, df_fe.columns != \"Class\"]\n",
    "y_rf = df_fe.iloc[:, df_fe.columns == \"Class\"]\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y_rf, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_rf(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    n_estimators_range = [10, 100, 150, 200]\n",
    "    max_features_type = ['auto', 'log2']\n",
    "    max_depth_range = [10, 20, 30, None]\n",
    "    n_estimators_list = []\n",
    "    max_features_list = []\n",
    "    max_depth_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'n_estimators: ', n_estimators\n",
    "        print '-------------------------------------------'\n",
    "        for max_features in max_features_type:\n",
    "            print '-------------------------------------------'\n",
    "            print 'max_features: ', max_features\n",
    "            print '-------------------------------------------'\n",
    "            for max_depth in max_depth_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'max_depth: ', max_depth\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    rf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(rf, X_train_data.iloc[train], X_test_rf, y_train_data.iloc[train], y_test_rf)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                n_estimators_list.append(n_estimators)\n",
    "                max_features_list.append(max_features)\n",
    "                max_depth_list.append(max_depth)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['n_estimators', 'max_features', 'max_depth', 'Mean loss'])\n",
    "    results_table['n_estimators'] = n_estimators_list\n",
    "    results_table['max_features'] = max_features_list\n",
    "    results_table['max_depth'] = max_depth_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['n_estimators']\n",
    "    best_max_features = results_table.loc[results_table['Mean loss'].idxmin()]['max_features']\n",
    "    best_max_depth = results_table.loc[results_table['Mean loss'].idxmin()]['max_depth']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with n_estimators = \", best_n_estimators, \", best max_features = \", best_max_features\n",
    "    print \"and best max_depth = \", best_max_depth\n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "n_estimators:  10\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.28242857142858\n",
      "Fold  2 : loss =  44.50055172413794\n",
      "Fold  3 : loss =  44.88126582278481\n",
      "Fold  4 : loss =  42.438458333333344\n",
      "Fold  5 : loss =  37.8631145038168\n",
      "\n",
      "Mean loss 42.3931637911003\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.99019607843138\n",
      "Fold  2 : loss =  32.8756170212766\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.71153034125831\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.72066666666668\n",
      "Fold  2 : loss =  32.39257142857142\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.36101534036434\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.72066666666668\n",
      "Fold  2 : loss =  32.39257142857142\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.36101534036434\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.28242857142858\n",
      "Fold  2 : loss =  44.50055172413794\n",
      "Fold  3 : loss =  44.88126582278481\n",
      "Fold  4 : loss =  42.438458333333344\n",
      "Fold  5 : loss =  37.8631145038168\n",
      "\n",
      "Mean loss 42.3931637911003\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.99019607843138\n",
      "Fold  2 : loss =  32.8756170212766\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.71153034125831\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.72066666666668\n",
      "Fold  2 : loss =  32.39257142857142\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.36101534036434\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.72066666666668\n",
      "Fold  2 : loss =  32.39257142857142\n",
      "Fold  3 : loss =  28.178500000000007\n",
      "Fold  4 : loss =  30.065695749440714\n",
      "Fold  5 : loss =  27.447642857142856\n",
      "\n",
      "Mean loss 30.36101534036434\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  100\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.215370927318304\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  36.04908457711443\n",
      "Fold  4 : loss =  36.697378787878804\n",
      "Fold  5 : loss =  36.17147837150128\n",
      "\n",
      "Mean loss 36.56613829033832\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.307925170068025\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.341775198461118\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.834840182648392\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  24.981793103448283\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.15207024632438\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.834840182648392\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  24.981793103448283\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.15207024632438\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.215370927318304\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  36.04908457711443\n",
      "Fold  4 : loss =  36.697378787878804\n",
      "Fold  5 : loss =  36.17147837150128\n",
      "\n",
      "Mean loss 36.56613829033832\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.307925170068025\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.341775198461118\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.834840182648392\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  24.981793103448283\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.15207024632438\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.834840182648392\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  24.981793103448283\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.15207024632438\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  150\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  36.04908457711443\n",
      "Fold  4 : loss =  37.72563184079603\n",
      "Fold  5 : loss =  35.01084848484849\n",
      "\n",
      "Mean loss 36.306405653550435\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  25.235044742729308\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.52719911299338\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  24.083066666666667\n",
      "Fold  4 : loss =  25.92620408163266\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.620555614089902\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  24.083066666666667\n",
      "Fold  4 : loss =  25.92620408163266\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.620555614089902\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  36.04908457711443\n",
      "Fold  4 : loss =  37.72563184079603\n",
      "Fold  5 : loss =  35.01084848484849\n",
      "\n",
      "Mean loss 36.306405653550435\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  25.235044742729308\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.52719911299338\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  24.083066666666667\n",
      "Fold  4 : loss =  25.92620408163266\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.620555614089902\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  24.083066666666667\n",
      "Fold  4 : loss =  25.92620408163266\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.620555614089902\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  200\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  35.53386967418547\n",
      "Fold  4 : loss =  37.72563184079603\n",
      "Fold  5 : loss =  35.01084848484849\n",
      "\n",
      "Mean loss 36.203362672964644\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.307925170068025\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.346645922648957\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  23.834840182648392\n",
      "\n",
      "Mean loss 24.34536731457485\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  23.834840182648392\n",
      "\n",
      "Mean loss 24.34536731457485\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  log2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  10\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.697378787878804\n",
      "Fold  3 : loss =  35.53386967418547\n",
      "Fold  4 : loss =  37.72563184079603\n",
      "Fold  5 : loss =  35.01084848484849\n",
      "\n",
      "Mean loss 36.203362672964644\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  20\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.307925170068025\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.346645922648957\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  30\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  23.834840182648392\n",
      "\n",
      "Mean loss 24.34536731457485\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  None\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.774617117117117\n",
      "Fold  2 : loss =  24.49975000000001\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  23.834840182648392\n",
      "\n",
      "Mean loss 24.34536731457485\n",
      "\n",
      "    n_estimators max_features  max_depth  Mean loss\n",
      "0             10         auto       10.0  42.393164\n",
      "1             10         auto       20.0  30.711530\n",
      "2             10         auto       30.0  30.361015\n",
      "3             10         auto        NaN  30.361015\n",
      "4             10         log2       10.0  42.393164\n",
      "5             10         log2       20.0  30.711530\n",
      "6             10         log2       30.0  30.361015\n",
      "7             10         log2        NaN  30.361015\n",
      "8            100         auto       10.0  36.566138\n",
      "9            100         auto       20.0  24.341775\n",
      "10           100         auto       30.0  24.152070\n",
      "11           100         auto        NaN  24.152070\n",
      "12           100         log2       10.0  36.566138\n",
      "13           100         log2       20.0  24.341775\n",
      "14           100         log2       30.0  24.152070\n",
      "15           100         log2        NaN  24.152070\n",
      "16           150         auto       10.0  36.306406\n",
      "17           150         auto       20.0  24.527199\n",
      "18           150         auto       30.0  24.620556\n",
      "19           150         auto        NaN  24.620556\n",
      "20           150         log2       10.0  36.306406\n",
      "21           150         log2       20.0  24.527199\n",
      "22           150         log2       30.0  24.620556\n",
      "23           150         log2        NaN  24.620556\n",
      "24           200         auto       10.0  36.203363\n",
      "25           200         auto       20.0  24.346646\n",
      "26           200         auto       30.0  24.345367\n",
      "27           200         auto        NaN  24.345367\n",
      "28           200         log2       10.0  36.203363\n",
      "29           200         log2       20.0  24.346646\n",
      "30           200         log2       30.0  24.345367\n",
      "31           200         log2        NaN  24.345367\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with n_estimators =  100 , best max_features =  auto and best max_depth =  30.0\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_rf = Kfold_tuning_rf(X_undersample_train_rf, y_undersample_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems tuning these 3 hyperparameters does nothing to the result. The optimal result is given by n_estimators=100, max_features='auto' and max_depth=30(same as None) which is the default model we used in the former part. Try out min_samples_split ,min_samples_leaf and other number of max_features.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_rf1(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    max_features_range = ['auto', 1, 2, 8]\n",
    "    min_samples_split_range = [2, 4, 8]\n",
    "    min_samples_leaf_range = [1, 2, 4]\n",
    "\n",
    "    max_features_list = []\n",
    "    min_samples_split_list = []\n",
    "    min_samples_leaf_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for max_features in max_features_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'max_features: ', max_features\n",
    "        print '-------------------------------------------'\n",
    "        for min_samples_split in min_samples_split_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'min_samples_split: ', min_samples_split\n",
    "            print '-------------------------------------------'\n",
    "            for min_samples_leaf in min_samples_leaf_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'min_samples_leaf: ', min_samples_leaf\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    rf = RandomForestClassifier(n_estimators=100, max_features=max_features, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(rf, X_train_data.iloc[train], X_test_rf, y_train_data.iloc[train], y_test_rf)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                max_features_list.append(max_features)\n",
    "                min_samples_split_list.append(min_samples_split)\n",
    "                min_samples_leaf_list.append(min_samples_leaf)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['max_features', 'min_samples_split', 'min_samples_leaf', 'Mean loss'])\n",
    "    results_table['max_features'] = max_features_list\n",
    "    results_table['min_samples_split'] = min_samples_split_list\n",
    "    results_table['min_samples_leaf'] = min_samples_leaf_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_max_features = results_table.loc[results_table['Mean loss'].idxmin()]['max_features']\n",
    "    best_min_samples_split = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_split']\n",
    "    best_min_samples_leaf = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_leaf']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with best max_features = \", best_max_features, \", best min_samples_split = \", best_min_samples_split\n",
    "    print \"and best min_samples_leaf = \", best_min_samples_leaf\n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_rf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "max_features:  auto\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.834840182648392\n",
      "Fold  2 : loss =  23.012595744680862\n",
      "Fold  3 : loss =  23.624827740492172\n",
      "Fold  4 : loss =  25.457232876712336\n",
      "Fold  5 : loss =  24.774617117117117\n",
      "\n",
      "Mean loss 24.14082273233018\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.17147837150128\n",
      "Fold  3 : loss =  37.54960097323601\n",
      "Fold  4 : loss =  39.211720194647214\n",
      "Fold  5 : loss =  36.04908457711443\n",
      "\n",
      "Mean loss 37.00619373872267\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  38.3839090909091\n",
      "Fold  2 : loss =  39.402179104477625\n",
      "Fold  3 : loss =  38.3839090909091\n",
      "Fold  4 : loss =  38.3839090909091\n",
      "Fold  5 : loss =  40.070439393939395\n",
      "\n",
      "Mean loss 38.92486915422886\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  24.307925170068025\n",
      "Fold  2 : loss =  25.154723004694834\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  25.92620408163266\n",
      "Fold  5 : loss =  30.78104081632653\n",
      "\n",
      "Mean loss 25.866057893823687\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.04908457711443\n",
      "Fold  2 : loss =  36.17147837150128\n",
      "Fold  3 : loss =  37.54960097323601\n",
      "Fold  4 : loss =  39.211720194647214\n",
      "Fold  5 : loss =  36.04908457711443\n",
      "\n",
      "Mean loss 37.00619373872267\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  38.3839090909091\n",
      "Fold  2 : loss =  39.402179104477625\n",
      "Fold  3 : loss =  38.3839090909091\n",
      "Fold  4 : loss =  38.3839090909091\n",
      "Fold  5 : loss =  40.070439393939395\n",
      "\n",
      "Mean loss 38.92486915422886\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.37782608695652\n",
      "Fold  2 : loss =  35.01084848484849\n",
      "Fold  3 : loss =  34.72037681159421\n",
      "Fold  4 : loss =  37.72563184079603\n",
      "Fold  5 : loss =  35.2082685851319\n",
      "\n",
      "Mean loss 35.80859036186543\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  38.89687218045114\n",
      "Fold  2 : loss =  37.3343076923077\n",
      "Fold  3 : loss =  39.402179104477625\n",
      "Fold  4 : loss =  38.89687218045114\n",
      "Fold  5 : loss =  37.8631145038168\n",
      "\n",
      "Mean loss 38.47866913230088\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  38.3839090909091\n",
      "Fold  2 : loss =  39.402179104477625\n",
      "Fold  3 : loss =  38.3839090909091\n",
      "Fold  4 : loss =  38.3839090909091\n",
      "Fold  5 : loss =  40.070439393939395\n",
      "\n",
      "Mean loss 38.92486915422886\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  1\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  20.388205882352946\n",
      "Fold  2 : loss =  20.928408759124096\n",
      "Fold  3 : loss =  19.266289537712893\n",
      "Fold  4 : loss =  21.51166666666667\n",
      "Fold  5 : loss =  22.055063725490193\n",
      "\n",
      "Mean loss 20.82992691426936\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.12737398373984\n",
      "Fold  2 : loss =  32.79177049180328\n",
      "Fold  3 : loss =  34.533125683060106\n",
      "Fold  4 : loss =  38.01583606557378\n",
      "Fold  5 : loss =  36.82175000000001\n",
      "\n",
      "Mean loss 35.457971244835406\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.572385964912286\n",
      "Fold  2 : loss =  37.353073446327684\n",
      "Fold  3 : loss =  35.06833333333333\n",
      "Fold  4 : loss =  39.1189604519774\n",
      "Fold  5 : loss =  34.45005602240896\n",
      "\n",
      "Mean loss 36.512561843791936\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.526666666666678\n",
      "Fold  2 : loss =  27.71474479166666\n",
      "Fold  3 : loss =  29.422177083333338\n",
      "Fold  4 : loss =  31.69104651162791\n",
      "Fold  5 : loss =  30.55933070866142\n",
      "\n",
      "Mean loss 29.182793152391206\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.12737398373984\n",
      "Fold  2 : loss =  32.79177049180328\n",
      "Fold  3 : loss =  34.533125683060106\n",
      "Fold  4 : loss =  38.01583606557378\n",
      "Fold  5 : loss =  36.82175000000001\n",
      "\n",
      "Mean loss 35.457971244835406\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.572385964912286\n",
      "Fold  2 : loss =  37.353073446327684\n",
      "Fold  3 : loss =  35.06833333333333\n",
      "Fold  4 : loss =  39.1189604519774\n",
      "Fold  5 : loss =  34.45005602240896\n",
      "\n",
      "Mean loss 36.512561843791936\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  32.79177049180328\n",
      "Fold  2 : loss =  34.56336000000001\n",
      "Fold  3 : loss =  34.533125683060106\n",
      "Fold  4 : loss =  36.274480874316936\n",
      "Fold  5 : loss =  36.287346666666664\n",
      "\n",
      "Mean loss 34.890016743169404\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.95407692307693\n",
      "Fold  2 : loss =  36.86284552845528\n",
      "Fold  3 : loss =  33.929055096418736\n",
      "Fold  4 : loss =  35.676391184573\n",
      "Fold  5 : loss =  36.82175000000001\n",
      "\n",
      "Mean loss 35.64882374650479\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.572385964912286\n",
      "Fold  2 : loss =  37.353073446327684\n",
      "Fold  3 : loss =  35.06833333333333\n",
      "Fold  4 : loss =  39.1189604519774\n",
      "Fold  5 : loss =  34.45005602240896\n",
      "\n",
      "Mean loss 36.512561843791936\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  20.85440476190476\n",
      "Fold  2 : loss =  23.012595744680862\n",
      "Fold  3 : loss =  20.74080652680653\n",
      "Fold  4 : loss =  23.51529577464789\n",
      "Fold  5 : loss =  24.49975000000001\n",
      "\n",
      "Mean loss 22.52457056160801\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  33.94066666666666\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  33.39313178294574\n",
      "Fold  4 : loss =  35.637487179487195\n",
      "Fold  5 : loss =  35.09521705426358\n",
      "\n",
      "Mean loss 35.185923437435996\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.69792125984252\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  36.797302325581406\n",
      "Fold  4 : loss =  39.12364829396324\n",
      "Fold  5 : loss =  37.95933854166667\n",
      "\n",
      "Mean loss 37.48826498497412\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  23.118231884057977\n",
      "Fold  2 : loss =  28.596882494004802\n",
      "Fold  3 : loss =  26.433130434782605\n",
      "Fold  4 : loss =  27.576885644768858\n",
      "Fold  5 : loss =  26.944035971223016\n",
      "\n",
      "Mean loss 26.533833285767447\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  33.94066666666666\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  33.39313178294574\n",
      "Fold  4 : loss =  35.637487179487195\n",
      "Fold  5 : loss =  35.09521705426358\n",
      "\n",
      "Mean loss 35.185923437435996\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.69792125984252\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  36.797302325581406\n",
      "Fold  4 : loss =  39.12364829396324\n",
      "Fold  5 : loss =  37.95933854166667\n",
      "\n",
      "Mean loss 37.48826498497412\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  34.54447395833334\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  35.09521705426358\n",
      "Fold  4 : loss =  37.95933854166667\n",
      "Fold  5 : loss =  36.797302325581406\n",
      "\n",
      "Mean loss 36.45188927673236\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.69792125984252\n",
      "Fold  2 : loss =  37.3343076923077\n",
      "Fold  3 : loss =  36.797302325581406\n",
      "Fold  4 : loss =  37.95933854166667\n",
      "Fold  5 : loss =  37.41078477690289\n",
      "\n",
      "Mean loss 37.03993091926024\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.69792125984252\n",
      "Fold  2 : loss =  37.8631145038168\n",
      "Fold  3 : loss =  36.797302325581406\n",
      "Fold  4 : loss =  39.12364829396324\n",
      "Fold  5 : loss =  37.95933854166667\n",
      "\n",
      "Mean loss 37.48826498497412\n",
      "\n",
      "-------------------------------------------\n",
      "max_features:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  25.235044742729308\n",
      "Fold  2 : loss =  25.457232876712336\n",
      "Fold  3 : loss =  23.1603963963964\n",
      "Fold  4 : loss =  24.49975000000001\n",
      "Fold  5 : loss =  24.774617117117117\n",
      "\n",
      "Mean loss 24.62540822659103\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.389926470588236\n",
      "Fold  2 : loss =  37.215370927318304\n",
      "Fold  3 : loss =  35.2082685851319\n",
      "Fold  4 : loss =  35.88748175182482\n",
      "Fold  5 : loss =  36.04908457711443\n",
      "\n",
      "Mean loss 35.95002646239554\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  40.8738394160584\n",
      "Fold  2 : loss =  39.900000000000006\n",
      "Fold  3 : loss =  40.8738394160584\n",
      "Fold  4 : loss =  40.3905\n",
      "Fold  5 : loss =  38.3839090909091\n",
      "\n",
      "Mean loss 40.084417584605184\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  26.579978070175436\n",
      "Fold  2 : loss =  24.010965034965043\n",
      "Fold  3 : loss =  23.834840182648392\n",
      "Fold  4 : loss =  24.49975000000001\n",
      "Fold  5 : loss =  24.307925170068025\n",
      "\n",
      "Mean loss 24.64669169157138\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  35.389926470588236\n",
      "Fold  2 : loss =  37.215370927318304\n",
      "Fold  3 : loss =  35.2082685851319\n",
      "Fold  4 : loss =  35.88748175182482\n",
      "Fold  5 : loss =  36.04908457711443\n",
      "\n",
      "Mean loss 35.95002646239554\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  40.8738394160584\n",
      "Fold  2 : loss =  39.900000000000006\n",
      "Fold  3 : loss =  40.8738394160584\n",
      "Fold  4 : loss =  40.3905\n",
      "Fold  5 : loss =  38.3839090909091\n",
      "\n",
      "Mean loss 40.084417584605184\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  1\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  32.65370833333334\n",
      "Fold  2 : loss =  33.06292753623188\n",
      "Fold  3 : loss =  31.022916666666674\n",
      "Fold  4 : loss =  33.82144055944056\n",
      "Fold  5 : loss =  33.06292753623188\n",
      "\n",
      "Mean loss 32.72478412638087\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  39.900000000000006\n",
      "Fold  2 : loss =  38.3839090909091\n",
      "Fold  3 : loss =  36.37782608695652\n",
      "Fold  4 : loss =  40.63411904761905\n",
      "Fold  5 : loss =  36.697378787878804\n",
      "\n",
      "Mean loss 38.39864660267269\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_leaf:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  40.8738394160584\n",
      "Fold  2 : loss =  39.900000000000006\n",
      "Fold  3 : loss =  40.8738394160584\n",
      "Fold  4 : loss =  40.3905\n",
      "Fold  5 : loss =  38.3839090909091\n",
      "\n",
      "Mean loss 40.084417584605184\n",
      "\n",
      "   max_features  min_samples_split  min_samples_leaf  Mean loss\n",
      "0          auto                  2                 1  24.140823\n",
      "1          auto                  2                 2  37.006194\n",
      "2          auto                  2                 4  38.924869\n",
      "3          auto                  4                 1  25.866058\n",
      "4          auto                  4                 2  37.006194\n",
      "5          auto                  4                 4  38.924869\n",
      "6          auto                  8                 1  35.808590\n",
      "7          auto                  8                 2  38.478669\n",
      "8          auto                  8                 4  38.924869\n",
      "9             1                  2                 1  20.829927\n",
      "10            1                  2                 2  35.457971\n",
      "11            1                  2                 4  36.512562\n",
      "12            1                  4                 1  29.182793\n",
      "13            1                  4                 2  35.457971\n",
      "14            1                  4                 4  36.512562\n",
      "15            1                  8                 1  34.890017\n",
      "16            1                  8                 2  35.648824\n",
      "17            1                  8                 4  36.512562\n",
      "18            2                  2                 1  22.524571\n",
      "19            2                  2                 2  35.185923\n",
      "20            2                  2                 4  37.488265\n",
      "21            2                  4                 1  26.533833\n",
      "22            2                  4                 2  35.185923\n",
      "23            2                  4                 4  37.488265\n",
      "24            2                  8                 1  36.451889\n",
      "25            2                  8                 2  37.039931\n",
      "26            2                  8                 4  37.488265\n",
      "27            8                  2                 1  24.625408\n",
      "28            8                  2                 2  35.950026\n",
      "29            8                  2                 4  40.084418\n",
      "30            8                  4                 1  24.646692\n",
      "31            8                  4                 2  35.950026\n",
      "32            8                  4                 4  40.084418\n",
      "33            8                  8                 1  32.724784\n",
      "34            8                  8                 2  38.398647\n",
      "35            8                  8                 4  40.084418\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with best max_features =  1 , best min_samples_split =  2\n",
      "and best min_samples_leaf =  1\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_rf1 = Kfold_tuning_rf1(X_undersample_train_rf, y_undersample_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the optimal min_samples_split and min_samples_leaf are all default value.\n",
    "* optimal max_features could not be decided by the above grid search because 1 is the lower bound of our max_features_range. Thus, more numbers less than 1 should be tried.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "mean_loss = []\n",
    "max_features_list = []\n",
    "for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    loss_list = []\n",
    "    for k, (train, test) in enumerate(fold.split(X_undersample_train_rf, y_undersample_train_rf)):\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_features=i, random_state=0)\n",
    "        loss = custom_loss_fuction(rf, X_undersample_train_rf.iloc[train], X_test_rf, y_undersample_train_rf.iloc[train], y_test_rf)\n",
    "        loss_list.append(loss)\n",
    "    mean_loss.append(np.mean(loss_list))\n",
    "    max_features_list.append(i)\n",
    "plt.plot(max_features_list, mean_loss, '-b')\n",
    "plt.ylabel('mean_loss')\n",
    "plt.xlabel('max_features_list')\n",
    "plt.show()\n",
    "\n",
    "min_loss = np.min(mean_loss)\n",
    "min_loss_index = mean_loss.index(min_loss)\n",
    "optimal_max_features = max_features_list[min_loss_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 20.82992691426936\n"
     ]
    }
   ],
   "source": [
    "print optimal_max_features, min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not any big difference here. The optimal max_features would still be selected as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Random Forest Model\n",
    "* n_estimators=100, max_features=1; max_depth, min_samples_split and min_samples_leaf equals to default value.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 40 proportion\n",
      "the recall for this model is : 0.9833333333333333\n",
      "The accuracy is : 0.9996488862672397\n",
      "TP 118\n",
      "TN 71059\n",
      "FP 23\n",
      "FN 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71082\n",
      "          1       0.84      0.98      0.90       120\n",
      "\n",
      "avg / total       1.00      1.00      1.00     71202\n",
      "\n",
      "The loss is :  16.437248226950363\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 40 proportion\"\n",
    "optimal_rf = RandomForestClassifier(n_estimators=100, max_features=1, random_state=0)\n",
    "prediction_algorithms(optimal_rf, X_undersample_train_rf, X_test_rf, y_undersample_train_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Tree\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the normal transacation proportion is : 0.9875\n",
      "the fraud transacation proportion is : 0.0125\n",
      "total number of record in resampled data is: 39360\n"
     ]
    }
   ],
   "source": [
    "df_gbt = df.loc[:, ['V11', 'V17', 'V14', 'V10', 'V12', 'V4', 'Class']]\n",
    "undersample_data_gbt = undersample(df_gbt, normal_indices,fraud_indices, 79)\n",
    "X_undersample_gbt = undersample_data_gbt.iloc[:, undersample_data_gbt.columns != \"Class\"]\n",
    "y_undersample_gbt = undersample_data_gbt.iloc[:, undersample_data_gbt.columns == \"Class\"]\n",
    "X_undersample_train_gbt, X_undersample_test_gbt, y_undersample_train_gbt, y_undersample_test_gbt = train_test_split(X_undersample_gbt, y_undersample_gbt, random_state=0)\n",
    "X_gbt = df_gbt.iloc[:, df_gbt.columns != \"Class\"]\n",
    "y_gbt = df_gbt.iloc[:, df_gbt.columns == \"Class\"]\n",
    "X_train_gbt, X_test_gbt, y_train_gbt, y_test_gbt = train_test_split(X_gbt, y_gbt, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed learning rate, tuning n_estimators, max_depth, min_samples_split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    n_estimators_range = [100, 200, 300]\n",
    "    max_depth_range = [3, 5, 7, 9]\n",
    "    min_samples_split_range = [2, 4, 8, 16]\n",
    "    n_estimators_list = []\n",
    "    max_depth_list = []\n",
    "    min_samples_split_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'n_estimators: ', n_estimators\n",
    "        print '-------------------------------------------'\n",
    "        for max_depth in max_depth_range:\n",
    "            print '-------------------------------------------'\n",
    "            print 'max_depth: ', max_depth\n",
    "            print '-------------------------------------------'\n",
    "            for min_samples_split in min_samples_split_range:\n",
    "                print '-------------------------------------------'\n",
    "                print 'min_samples_split: ', min_samples_split\n",
    "                print '-------------------------------------------'\n",
    "\n",
    "                loss_list = []\n",
    "                for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "                    gbt = GradientBoostingClassifier(n_estimators=n_estimators,max_depth=max_depth, min_samples_split=min_samples_split, random_state=0)\n",
    "\n",
    "                    loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "                    loss_list.append(loss)\n",
    "                    print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "                j += 1\n",
    "                print ''\n",
    "                print 'Mean loss', np.mean(loss_list)\n",
    "                print ''\n",
    "                n_estimators_list.append(n_estimators)\n",
    "                max_depth_list.append(max_depth)\n",
    "                min_samples_split_list.append(min_samples_split)\n",
    "                mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['n_estimators', 'max_depth', 'min_samples_split', 'Mean loss'])\n",
    "    results_table['n_estimators'] = n_estimators_list\n",
    "    results_table['max_depth'] = max_depth_list\n",
    "    results_table['min_samples_split'] = min_samples_split_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "        \n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['n_estimators']\n",
    "    best_max_depth = results_table.loc[results_table['Mean loss'].idxmin()]['max_depth']\n",
    "    best_min_samples_split = results_table.loc[results_table['Mean loss'].idxmin()]['min_samples_split']\n",
    "    \n",
    "    print results_table\n",
    "    print  ''\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '**************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with n_estimators = \", best_n_estimators, \"and best max_depth = \", best_max_depth\n",
    "    print \"and best min_samples_split = \", best_min_samples_split \n",
    "    print '**************************************************************************************************'\n",
    "    \n",
    "    return Kfold_tuning_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "n_estimators:  100\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.696056497175135\n",
      "Fold  2 : loss =  61.09985123966941\n",
      "Fold  3 : loss =  57.32602898550725\n",
      "Fold  4 : loss =  60.6234329004329\n",
      "Fold  5 : loss =  58.96547747747748\n",
      "\n",
      "Mean loss 58.14216942005244\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  55.897054644808755\n",
      "Fold  2 : loss =  57.15963636363636\n",
      "Fold  3 : loss =  59.607333333333344\n",
      "Fold  4 : loss =  57.758673046252\n",
      "Fold  5 : loss =  56.96172602739726\n",
      "\n",
      "Mean loss 57.476884683085544\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  58.98552884615385\n",
      "Fold  2 : loss =  64.65493233082708\n",
      "Fold  3 : loss =  56.318566188197764\n",
      "Fold  4 : loss =  62.712907801418446\n",
      "Fold  5 : loss =  56.95820440251571\n",
      "\n",
      "Mean loss 59.926027913822566\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  53.71387096774193\n",
      "Fold  2 : loss =  58.00041666666668\n",
      "Fold  3 : loss =  50.881988636363644\n",
      "Fold  4 : loss =  59.562078431372555\n",
      "Fold  5 : loss =  56.73454545454546\n",
      "\n",
      "Mean loss 55.77858003133805\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  51.0026018957346\n",
      "Fold  2 : loss =  50.223482866043625\n",
      "Fold  3 : loss =  59.39582771535581\n",
      "Fold  4 : loss =  52.494\n",
      "Fold  5 : loss =  50.10508695652173\n",
      "\n",
      "Mean loss 52.64419988673116\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  53.4379420289855\n",
      "Fold  2 : loss =  49.56649921011058\n",
      "Fold  3 : loss =  52.28633333333333\n",
      "Fold  4 : loss =  55.02393506493506\n",
      "Fold  5 : loss =  55.20680172413793\n",
      "\n",
      "Mean loss 53.104302272300494\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.088373692077724\n",
      "Fold  2 : loss =  53.4379420289855\n",
      "Fold  3 : loss =  51.846985443959255\n",
      "Fold  4 : loss =  56.4440209205021\n",
      "Fold  5 : loss =  54.83947826086957\n",
      "\n",
      "Mean loss 53.731360069278836\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  54.70778902953586\n",
      "Fold  2 : loss =  51.278456621004565\n",
      "Fold  3 : loss =  48.79324610591901\n",
      "Fold  4 : loss =  51.86672093023256\n",
      "Fold  5 : loss =  56.784311203319504\n",
      "\n",
      "Mean loss 52.6861047780023\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  50.55813875598086\n",
      "Fold  2 : loss =  50.43840310077519\n",
      "Fold  3 : loss =  47.677925039872406\n",
      "Fold  4 : loss =  49.87526213592233\n",
      "Fold  5 : loss =  47.95309090909092\n",
      "\n",
      "Mean loss 49.30056398832834\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  51.653719626168225\n",
      "Fold  2 : loss =  48.66089855072464\n",
      "Fold  3 : loss =  47.44842628205129\n",
      "Fold  4 : loss =  50.78142857142856\n",
      "Fold  5 : loss =  54.83947826086957\n",
      "\n",
      "Mean loss 50.67679025824846\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  56.09798734177214\n",
      "Fold  2 : loss =  54.70778902953586\n",
      "Fold  3 : loss =  49.22491666666667\n",
      "Fold  4 : loss =  51.653719626168225\n",
      "Fold  5 : loss =  57.93210483870968\n",
      "\n",
      "Mean loss 53.923303500570526\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.86974743024963\n",
      "Fold  2 : loss =  51.278456621004565\n",
      "Fold  3 : loss =  50.65133333333334\n",
      "Fold  4 : loss =  51.86672093023256\n",
      "Fold  5 : loss =  55.20680172413793\n",
      "\n",
      "Mean loss 52.37461200779161\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.76610227272727\n",
      "Fold  2 : loss =  47.19161538461539\n",
      "Fold  3 : loss =  34.948393939393945\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  43.601802197802186\n",
      "\n",
      "Mean loss 41.52917223259197\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.39175280898877\n",
      "Fold  2 : loss =  48.145941278065635\n",
      "Fold  3 : loss =  35.672528942115775\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  45.041566844919785\n",
      "\n",
      "Mean loss 42.277947448502196\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.701842639593906\n",
      "Fold  2 : loss =  61.7446862745098\n",
      "Fold  3 : loss =  41.79884714548804\n",
      "Fold  4 : loss =  42.08069491525424\n",
      "Fold  5 : loss =  52.69927397260274\n",
      "\n",
      "Mean loss 49.205068989489746\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.343333333333334\n",
      "Fold  2 : loss =  53.25019796215429\n",
      "Fold  3 : loss =  41.192090579710154\n",
      "Fold  4 : loss =  47.701842639593906\n",
      "Fold  5 : loss =  47.44803061224489\n",
      "\n",
      "Mean loss 47.787099025407315\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  200\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  51.60292307692308\n",
      "Fold  2 : loss =  60.46715094339622\n",
      "Fold  3 : loss =  51.9982456140351\n",
      "Fold  4 : loss =  58.181286123032905\n",
      "Fold  5 : loss =  57.31454802259888\n",
      "\n",
      "Mean loss 55.91283075599724\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  57.433358208955234\n",
      "Fold  2 : loss =  56.09798734177214\n",
      "Fold  3 : loss =  52.6558176100629\n",
      "Fold  4 : loss =  57.45983551673945\n",
      "Fold  5 : loss =  57.48818565400845\n",
      "\n",
      "Mean loss 56.22703686630763\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  58.68042105263159\n",
      "Fold  2 : loss =  64.00125255972696\n",
      "Fold  3 : loss =  57.31454802259888\n",
      "Fold  4 : loss =  61.99234449093444\n",
      "Fold  5 : loss =  58.87838396624473\n",
      "\n",
      "Mean loss 60.17339001842733\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  53.216977564102564\n",
      "Fold  2 : loss =  56.4440209205021\n",
      "Fold  3 : loss =  49.16768189509307\n",
      "Fold  4 : loss =  59.3454255952381\n",
      "Fold  5 : loss =  57.35575565610859\n",
      "\n",
      "Mean loss 55.10597232620889\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.17220197044335\n",
      "Fold  2 : loss =  43.83303546099292\n",
      "Fold  3 : loss =  56.87741333333334\n",
      "Fold  4 : loss =  48.448049999999995\n",
      "Fold  5 : loss =  46.67081347150258\n",
      "\n",
      "Mean loss 49.00030284725444\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.088373692077724\n",
      "Fold  2 : loss =  44.111333333333334\n",
      "Fold  3 : loss =  48.79324610591901\n",
      "Fold  4 : loss =  54.08532743362832\n",
      "Fold  5 : loss =  51.221688679245275\n",
      "\n",
      "Mean loss 50.05999384884073\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.56649921011058\n",
      "Fold  2 : loss =  51.88863063063063\n",
      "Fold  3 : loss =  49.22491666666667\n",
      "Fold  4 : loss =  55.38809871244635\n",
      "Fold  5 : loss =  53.304\n",
      "\n",
      "Mean loss 51.87442904397085\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 : loss =  51.07133333333333\n",
      "Fold  2 : loss =  48.89056410256411\n",
      "Fold  3 : loss =  46.26701642036126\n",
      "Fold  4 : loss =  49.40885294117648\n",
      "Fold  5 : loss =  53.89259999999999\n",
      "\n",
      "Mean loss 49.90607335948703\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.041566844919785\n",
      "Fold  2 : loss =  43.83303546099292\n",
      "Fold  3 : loss =  43.72055785837652\n",
      "Fold  4 : loss =  47.19161538461539\n",
      "Fold  5 : loss =  46.67081347150258\n",
      "\n",
      "Mean loss 45.29151780408144\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.19161538461539\n",
      "Fold  2 : loss =  41.495333333333335\n",
      "Fold  3 : loss =  44.92883333333333\n",
      "Fold  4 : loss =  48.448049999999995\n",
      "Fold  5 : loss =  48.448049999999995\n",
      "\n",
      "Mean loss 46.10237641025641\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  53.89259999999999\n",
      "Fold  2 : loss =  52.86974743024963\n",
      "Fold  3 : loss =  45.27914405360134\n",
      "Fold  4 : loss =  50.332701923076925\n",
      "Fold  5 : loss =  57.61082926829269\n",
      "\n",
      "Mean loss 51.997004535044105\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  50.43840310077519\n",
      "Fold  2 : loss =  49.11803189792663\n",
      "Fold  3 : loss =  49.78755974842767\n",
      "Fold  4 : loss =  48.448049999999995\n",
      "Fold  5 : loss =  51.438718309859155\n",
      "\n",
      "Mean loss 49.84615261139773\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.44791428571429\n",
      "Fold  2 : loss =  46.406343750000005\n",
      "Fold  3 : loss =  34.948393939393945\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  43.601802197802186\n",
      "\n",
      "Mean loss 41.3084803082663\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.08069491525424\n",
      "Fold  2 : loss =  47.88385416666667\n",
      "Fold  3 : loss =  35.31264257028113\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  43.601802197802186\n",
      "\n",
      "Mean loss 41.803388243685056\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.701842639593906\n",
      "Fold  2 : loss =  61.59610498687664\n",
      "Fold  3 : loss =  40.564214689265526\n",
      "Fold  4 : loss =  42.08069491525424\n",
      "Fold  5 : loss =  52.69927397260274\n",
      "\n",
      "Mean loss 48.928426240718615\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.66089855072464\n",
      "Fold  2 : loss =  51.07133333333333\n",
      "Fold  3 : loss =  37.40918992248063\n",
      "Fold  4 : loss =  46.93255670103093\n",
      "Fold  5 : loss =  45.041566844919785\n",
      "\n",
      "Mean loss 45.823109070497864\n",
      "\n",
      "-------------------------------------------\n",
      "n_estimators:  300\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "max_depth:  3\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  50.633521150592216\n",
      "Fold  2 : loss =  58.68904325699745\n",
      "Fold  3 : loss =  49.40885294117648\n",
      "Fold  4 : loss =  57.82368398268398\n",
      "Fold  5 : loss =  57.31454802259888\n",
      "\n",
      "Mean loss 54.773929870809795\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  52.76777993527509\n",
      "Fold  2 : loss =  53.99150500715308\n",
      "Fold  3 : loss =  51.86672093023256\n",
      "Fold  4 : loss =  55.87061403508773\n",
      "Fold  5 : loss =  56.24101449275363\n",
      "\n",
      "Mean loss 54.147526880100415\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  57.08957562408224\n",
      "Fold  2 : loss =  60.98875268817204\n",
      "Fold  3 : loss =  55.22933333333333\n",
      "Fold  4 : loss =  60.09810169491525\n",
      "Fold  5 : loss =  57.45983551673945\n",
      "\n",
      "Mean loss 58.17311977144847\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  54.089946540880504\n",
      "Fold  2 : loss =  55.22933333333333\n",
      "Fold  3 : loss =  48.66205128205129\n",
      "Fold  4 : loss =  57.550108108108105\n",
      "Fold  5 : loss =  55.109970238095244\n",
      "\n",
      "Mean loss 54.1282819004937\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  5\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.448049999999995\n",
      "Fold  2 : loss =  41.495333333333335\n",
      "Fold  3 : loss =  56.397001349527656\n",
      "Fold  4 : loss =  46.93255670103093\n",
      "Fold  5 : loss =  46.67081347150258\n",
      "\n",
      "Mean loss 47.9887509710789\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  51.48369696969698\n",
      "Fold  2 : loss =  41.495333333333335\n",
      "Fold  3 : loss =  47.677925039872406\n",
      "Fold  4 : loss =  53.89259999999999\n",
      "Fold  5 : loss =  49.17220197044335\n",
      "\n",
      "Mean loss 48.744351462669215\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.89056410256411\n",
      "Fold  2 : loss =  52.088373692077724\n",
      "Fold  3 : loss =  48.35343081761006\n",
      "Fold  4 : loss =  55.20680172413793\n",
      "Fold  5 : loss =  52.28683410138248\n",
      "\n",
      "Mean loss 51.365200887554465\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  49.56649921011058\n",
      "Fold  2 : loss =  48.89056410256411\n",
      "Fold  3 : loss =  45.529816666666676\n",
      "Fold  4 : loss =  48.93320792079207\n",
      "Fold  5 : loss =  52.07775000000001\n",
      "\n",
      "Mean loss 48.99956758002669\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  45.041566844919785\n",
      "Fold  2 : loss =  43.83303546099292\n",
      "Fold  3 : loss =  43.72055785837652\n",
      "Fold  4 : loss =  47.19161538461539\n",
      "Fold  5 : loss =  46.67081347150258\n",
      "\n",
      "Mean loss 45.29151780408144\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.19161538461539\n",
      "Fold  2 : loss =  41.495333333333335\n",
      "Fold  3 : loss =  44.92883333333333\n",
      "Fold  4 : loss =  48.448049999999995\n",
      "Fold  5 : loss =  48.448049999999995\n",
      "\n",
      "Mean loss 46.10237641025641\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  53.89259999999999\n",
      "Fold  2 : loss =  52.86974743024963\n",
      "Fold  3 : loss =  45.27914405360134\n",
      "Fold  4 : loss =  50.332701923076925\n",
      "Fold  5 : loss =  57.61082926829269\n",
      "\n",
      "Mean loss 51.997004535044105\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  50.43840310077519\n",
      "Fold  2 : loss =  49.11803189792663\n",
      "Fold  3 : loss =  49.78755974842767\n",
      "Fold  4 : loss =  48.448049999999995\n",
      "Fold  5 : loss =  51.438718309859155\n",
      "\n",
      "Mean loss 49.84615261139773\n",
      "\n",
      "-------------------------------------------\n",
      "max_depth:  9\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "min_samples_split:  2\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.44791428571429\n",
      "Fold  2 : loss =  46.406343750000005\n",
      "Fold  3 : loss =  34.948393939393945\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  43.601802197802186\n",
      "\n",
      "Mean loss 41.3084803082663\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  4\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.08069491525424\n",
      "Fold  2 : loss =  47.88385416666667\n",
      "Fold  3 : loss =  35.31264257028113\n",
      "Fold  4 : loss =  40.13794736842105\n",
      "Fold  5 : loss =  43.601802197802186\n",
      "\n",
      "Mean loss 41.803388243685056\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  47.701842639593906\n",
      "Fold  2 : loss =  61.59610498687664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3 : loss =  40.564214689265526\n",
      "Fold  4 : loss =  42.08069491525424\n",
      "Fold  5 : loss =  52.69927397260274\n",
      "\n",
      "Mean loss 48.928426240718615\n",
      "\n",
      "-------------------------------------------\n",
      "min_samples_split:  16\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.66089855072464\n",
      "Fold  2 : loss =  51.07133333333333\n",
      "Fold  3 : loss =  37.40918992248063\n",
      "Fold  4 : loss =  46.93255670103093\n",
      "Fold  5 : loss =  45.041566844919785\n",
      "\n",
      "Mean loss 45.823109070497864\n",
      "\n",
      "    n_estimators  max_depth  min_samples_split  Mean loss\n",
      "0            100          3                  2  58.142169\n",
      "1            100          3                  4  57.476885\n",
      "2            100          3                  8  59.926028\n",
      "3            100          3                 16  55.778580\n",
      "4            100          5                  2  52.644200\n",
      "5            100          5                  4  53.104302\n",
      "6            100          5                  8  53.731360\n",
      "7            100          5                 16  52.686105\n",
      "8            100          7                  2  49.300564\n",
      "9            100          7                  4  50.676790\n",
      "10           100          7                  8  53.923304\n",
      "11           100          7                 16  52.374612\n",
      "12           100          9                  2  41.529172\n",
      "13           100          9                  4  42.277947\n",
      "14           100          9                  8  49.205069\n",
      "15           100          9                 16  47.787099\n",
      "16           200          3                  2  55.912831\n",
      "17           200          3                  4  56.227037\n",
      "18           200          3                  8  60.173390\n",
      "19           200          3                 16  55.105972\n",
      "20           200          5                  2  49.000303\n",
      "21           200          5                  4  50.059994\n",
      "22           200          5                  8  51.874429\n",
      "23           200          5                 16  49.906073\n",
      "24           200          7                  2  45.291518\n",
      "25           200          7                  4  46.102376\n",
      "26           200          7                  8  51.997005\n",
      "27           200          7                 16  49.846153\n",
      "28           200          9                  2  41.308480\n",
      "29           200          9                  4  41.803388\n",
      "30           200          9                  8  48.928426\n",
      "31           200          9                 16  45.823109\n",
      "32           300          3                  2  54.773930\n",
      "33           300          3                  4  54.147527\n",
      "34           300          3                  8  58.173120\n",
      "35           300          3                 16  54.128282\n",
      "36           300          5                  2  47.988751\n",
      "37           300          5                  4  48.744351\n",
      "38           300          5                  8  51.365201\n",
      "39           300          5                 16  48.999568\n",
      "40           300          7                  2  45.291518\n",
      "41           300          7                  4  46.102376\n",
      "42           300          7                  8  51.997005\n",
      "43           300          7                 16  49.846153\n",
      "44           300          9                  2  41.308480\n",
      "45           300          9                  4  41.803388\n",
      "46           300          9                  8  48.928426\n",
      "47           300          9                 16  45.823109\n",
      "\n",
      "**************************************************************************************************\n",
      "Best model to choose from cross validation is with n_estimators =  200.0 and best max_depth =  9.0\n",
      "and best min_samples_split =  2.0\n",
      "**************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt = Kfold_tuning_gbt(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal n_estimators and min_samples_split are 200 and 2(default).\n",
    "* The optimal max_depth could not be decided by the above grid search because 9 is the upper bound of our max_depth_range. Thus, more numbers larger than 9 should be tried.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "mean_loss = []\n",
    "max_depth_list = []\n",
    "for i in range(9, 33, 2):\n",
    "    loss_list = []\n",
    "    for k, (train, test) in enumerate(fold.split(X_undersample_train_gbt, y_undersample_train_gbt)):\n",
    "        gbt = GradientBoostingClassifier(n_estimators=200, max_depth=i)\n",
    "        loss = custom_loss_fuction(gbt, X_undersample_train_gbt.iloc[train], X_test_gbt, y_undersample_train_gbt.iloc[train], y_test_gbt)\n",
    "        loss_list.append(loss)\n",
    "    mean_loss.append(np.mean(loss_list))\n",
    "    max_depth_list.append(i)\n",
    "plt.plot(max_depth_list, mean_loss, '-b')\n",
    "plt.ylabel('mean_loss')\n",
    "plt.xlabel('max_depth_list')\n",
    "plt.show()\n",
    "\n",
    "min_loss = np.min(mean_loss)\n",
    "min_loss_index = mean_loss.index(min_loss)\n",
    "optimal_max_depth = max_depth_list[min_loss_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 39.439148611761915)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is the optimal max_depth\n",
    "optimal_max_depth, min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, we got optimal n_estimators=200, max_depth=11, min_samples_split=2(default).\n",
    "#### Tuning subsample and making models with lower learning rate.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt1(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    subsample_range = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "    subsample_list = []\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for subsample in subsample_range:\n",
    "        print '-------------------------------------------'\n",
    "        print 'subsample: ', subsample\n",
    "        print '-------------------------------------------'\n",
    "        loss_list = []\n",
    "        for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "            gbt = GradientBoostingClassifier(n_estimators=200,max_depth=11, subsample=subsample, random_state=0)\n",
    "\n",
    "            loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "            loss_list.append(loss)\n",
    "            print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "        j += 1\n",
    "        print ''\n",
    "        print 'Mean loss', np.mean(loss_list)\n",
    "        print ''\n",
    "        subsample_list.append(subsample)\n",
    "        mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Subsample','Mean loss'])\n",
    "    results_table['Subsample'] = subsample_list\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "\n",
    "    best_subsample = results_table.loc[results_table['Mean loss'].idxmin()]['Subsample']\n",
    "\n",
    "    print results_table\n",
    "    print  ''\n",
    "\n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '*******************************************************************'\n",
    "    print \"Best model to choose from cross validation is with subsample = \", best_subsample\n",
    "    print '*******************************************************************'\n",
    "\n",
    "    return Kfold_tuning_gbt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "subsample:  0.6\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  48.448049999999995\n",
      "Fold  2 : loss =  34.62833333333334\n",
      "Fold  3 : loss =  48.91615646258504\n",
      "Fold  4 : loss =  47.701842639593906\n",
      "Fold  5 : loss =  53.50195964125561\n",
      "\n",
      "Mean loss 46.63926841535358\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.65\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  42.345741134751776\n",
      "Fold  2 : loss =  49.64319512195121\n",
      "Fold  3 : loss =  53.89259999999999\n",
      "Fold  4 : loss =  46.252150537634414\n",
      "Fold  5 : loss =  53.50416666666667\n",
      "\n",
      "Mean loss 49.12757069220081\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.7\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  44.38670175438597\n",
      "Fold  2 : loss =  43.55176114081996\n",
      "Fold  3 : loss =  42.97009523809525\n",
      "Fold  4 : loss =  48.145941278065635\n",
      "Fold  5 : loss =  47.08095238095239\n",
      "\n",
      "Mean loss 45.22709035846384\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.75\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  43.55176114081996\n",
      "Fold  2 : loss =  45.969909909909916\n",
      "Fold  3 : loss =  42.874862745098035\n",
      "Fold  4 : loss =  46.406343750000005\n",
      "Fold  5 : loss =  45.32032978723403\n",
      "\n",
      "Mean loss 44.82464146661239\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.8\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  40.564214689265526\n",
      "Fold  2 : loss =  43.00349999999999\n",
      "Fold  3 : loss =  50.13248717948718\n",
      "Fold  4 : loss =  42.08069491525424\n",
      "Fold  5 : loss =  48.448049999999995\n",
      "\n",
      "Mean loss 44.84578935680138\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.85\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  41.76610227272727\n",
      "Fold  2 : loss =  42.39175280898877\n",
      "Fold  3 : loss =  43.856529865125246\n",
      "Fold  4 : loss =  36.57825465838509\n",
      "Fold  5 : loss =  45.32032978723403\n",
      "\n",
      "Mean loss 41.98259387849208\n",
      "\n",
      "-------------------------------------------\n",
      "subsample:  0.9\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  37.91961735700197\n",
      "Fold  2 : loss =  37.57133333333334\n",
      "Fold  3 : loss =  42.200940476190475\n",
      "Fold  4 : loss =  37.691743902439015\n",
      "Fold  5 : loss =  47.44803061224489\n",
      "\n",
      "Mean loss 40.56633313624194\n",
      "\n",
      "   Subsample  Mean loss\n",
      "0       0.60  46.639268\n",
      "1       0.65  49.127571\n",
      "2       0.70  45.227090\n",
      "3       0.75  44.824641\n",
      "4       0.80  44.845789\n",
      "5       0.85  41.982594\n",
      "6       0.90  40.566333\n",
      "\n",
      "*******************************************************************\n",
      "Best model to choose from cross validation is with subsample =  0.9\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt1 = Kfold_tuning_gbt1(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When subsample = 0.9, the mean loss is 40.56 which is larger than the default one(39.44). Thus, the optimal subsample=default(1.0).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_tuning_gbt2(X_train_data, y_train_data):\n",
    "    fold = KFold(n_splits=5, shuffle=False, random_state=0) \n",
    "\n",
    "    learning_rate_range = [0.1, 0.05, 0.01, 0.005]\n",
    "    n_estimators_range = [200, 400, 2000, 4000]\n",
    "    mean_loss_list = []\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0,4):\n",
    "        print '-------------------------------------------'\n",
    "        print 'learning_rate: ', learning_rate_range[i]\n",
    "        print 'n_estimators: ', n_estimators_range[i]\n",
    "        print '-------------------------------------------'\n",
    "        loss_list = []\n",
    "        for k, (train, test) in enumerate(fold.split(X_train_data, y_train_data)):\n",
    "\n",
    "            gbt = GradientBoostingClassifier(learning_rate=learning_rate_range[i], n_estimators=n_estimators_range[i],max_depth=11, random_state=0)\n",
    "\n",
    "            loss = custom_loss_fuction(gbt, X_train_data.iloc[train], X_test_gbt, y_train_data.iloc[train], y_test_gbt)\n",
    "            loss_list.append(loss)\n",
    "            print 'Fold ', k + 1,': loss = ', loss\n",
    "\n",
    "        j += 1\n",
    "        print ''\n",
    "        print 'Mean loss', np.mean(loss_list)\n",
    "        print ''\n",
    "        mean_loss_list.append(np.mean(loss_list))\n",
    "    results_table = pd.DataFrame(index = range(len(mean_loss_list)), columns = ['Learning_rate', 'N_estimators', 'Mean loss'])\n",
    "    results_table['Learning_rate'] = learning_rate_range\n",
    "    results_table['N_estimators'] = n_estimators_range\n",
    "    results_table['Mean loss'] = mean_loss_list\n",
    "    \n",
    "    best_learning_rate = results_table.loc[results_table['Mean loss'].idxmin()]['Learning_rate']\n",
    "    best_n_estimators = results_table.loc[results_table['Mean loss'].idxmin()]['N_estimators']\n",
    "\n",
    "    print results_table\n",
    "    print  ''\n",
    "\n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print '*******************************************************************************************************************'\n",
    "    print \"Best model to choose from cross validation is with best learning rate = \", best_learning_rate, \" and corresponding n_estimators = \", best_n_estimators\n",
    "    print '*******************************************************************************************************************'\n",
    "\n",
    "\n",
    "    return Kfold_tuning_gbt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "learning_rate:  0.1\n",
      "n_estimators:  200\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  44.474999999999994\n",
      "Fold  2 : loss =  34.99019607843138\n",
      "Fold  3 : loss =  34.57970325203253\n",
      "Fold  4 : loss =  39.11453571428571\n",
      "Fold  5 : loss =  46.67081347150258\n",
      "\n",
      "Mean loss 39.96604970325044\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.05\n",
      "n_estimators:  400\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  36.57825465838509\n",
      "Fold  2 : loss =  30.55128205128205\n",
      "Fold  3 : loss =  31.047408602150544\n",
      "Fold  4 : loss =  29.86148275862069\n",
      "Fold  5 : loss =  39.80082352941176\n",
      "\n",
      "Mean loss 33.56785031997003\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.01\n",
      "n_estimators:  2000\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  29.392124999999993\n",
      "Fold  2 : loss =  30.07300469483569\n",
      "Fold  3 : loss =  29.342342163355408\n",
      "Fold  4 : loss =  29.392124999999993\n",
      "Fold  5 : loss =  41.126068965517234\n",
      "\n",
      "Mean loss 31.865133164741668\n",
      "\n",
      "-------------------------------------------\n",
      "learning_rate:  0.005\n",
      "n_estimators:  4000\n",
      "-------------------------------------------\n",
      "Fold  1 : loss =  29.86148275862069\n",
      "Fold  2 : loss =  29.09595238095239\n",
      "Fold  3 : loss =  32.974065789473684\n",
      "Fold  4 : loss =  29.392124999999993\n",
      "Fold  5 : loss =  41.126068965517234\n",
      "\n",
      "Mean loss 32.489938978912804\n",
      "\n",
      "   Learning_rate  N_estimators  Mean loss\n",
      "0          0.100           200  39.966050\n",
      "1          0.050           400  33.567850\n",
      "2          0.010          2000  31.865133\n",
      "3          0.005          4000  32.489939\n",
      "\n",
      "*******************************************************************************************************************\n",
      "Best model to choose from cross validation is with best learning rate =  0.01  and corresponding n_estimators =  2000.0\n",
      "*******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_parameters_gbt2 = Kfold_tuning_gbt2(X_undersample_train_gbt, y_undersample_train_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal GBT Model\n",
    "* leanring_rate = 0.01, n_estimators = 2000, max_depth=11; min_samples_split, subsample equals to default.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model classification for 79 proportion\n",
      "the recall for this model is : 0.9583333333333334\n",
      "The accuracy is : 0.9994663071262043\n",
      "TP 115\n",
      "TN 71049\n",
      "FP 33\n",
      "FN 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     71082\n",
      "          1       0.78      0.96      0.86       120\n",
      "\n",
      "avg / total       1.00      1.00      1.00     71202\n",
      "\n",
      "The loss is :  24.774617117117117\n"
     ]
    }
   ],
   "source": [
    "print \"the model classification for 79 proportion\"\n",
    "optimal_gbt = GradientBoostingClassifier(learning_rate=0.01, n_estimators=2000, max_depth=11, random_state=0)\n",
    "prediction_algorithms(optimal_gbt, X_undersample_train_gbt, X_test_gbt, y_undersample_train_gbt, y_test_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of model selection\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model performances without resampling, pre-precessing or tunning hyperparameters:\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|\n",
    "|   Logistic Regression  |    0.81   |  0.60  | 0.69 |   0.99   |    65.62    |\n",
    "| Support Vector Machine |    1.00   |  0.03  | 0.05 |   0.99   |    118.46   |\n",
    "|      *Random Forest    |    0.94   |  0.75  | 0.83 |   0.99   |    35.83    | \n",
    "| Gradient Boosting Tree |    0.50   |  0.47  | 0.60 |   0.99   |    108.87   |\n",
    "\n",
    "#### Optimal model performances after pre-processing and selection of optimal proportion:\n",
    "\n",
    "* All the models perform better after selecting a appropriate proportion for undersampling especially for SVM and GBT.\n",
    "\n",
    "* Random Forest is the best among the 4 models.\n",
    "\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss | Diff vs 1st table |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|:-----------------:|\n",
    "|   Logistic Regression  |    0.71   |  0.74  | 0.73 |   0.99   |    57.35    |       -8.27      |\n",
    "| Support Vector Machine |    1.00   |  0.78  | 0.88 |   0.99   |    26.87    |       -91.59      |\n",
    "|      *Random Forest    |    0.81   |  0.98  | 0.89 |   0.99   |    19.22    |       -16.61      |\n",
    "| Gradient Boosting Tree |    0.52   |  0.94  | 0.67 |   0.99   |    49.71    |       -59.16      |\n",
    "\n",
    "#### Optimal model performances after tunning hyperparameters:\n",
    "\n",
    "* All the models have got improvements from hyperparameters' tunning especially for Logistic Regression and Gradient Boosting Tree.\n",
    "\n",
    "* In conclusion, SVM and Random Forest are more sensitive to the proportion of undersampling compared to tunning hyperparameters. Logistic Reregssion and GBT are sensetive to both of them.\n",
    "\n",
    "* Random Forest is still the best amoung the 4 models. Actually the improvement after tunning hyperparameters is trivial for Support Vector Machine (non-linear) and Random Forest. Anyway, better than nothing. It still saves about $ 2.65 per (fraud + misclassification normal) transaction for us.\n",
    "\n",
    "|         Models         | Precision | Recall |  F1  | Accuracy | Custom Loss | Diff vs 1st table | Diff vs 2nd table |\n",
    "|:----------------------:|:---------:|:------:|:----:|:--------:|:-----------:|:-----------------:|:-----------------:|\n",
    "|   Logistic Regression  |    0.84   |  0.79  | 0.82 |   0.99   |    39.78    |       -25.84      |       -17.57      |\n",
    "| Support Vector Machine |    1.00   |  0.79  | 0.88 |   0.99   |    25.65    |       -92.81      |       -1.22       |\n",
    "|      Random Forest     |    0.84   |  0.98  | 0.89 |   0.99   |    16.57    |       -19.26      |       -2.65       |\n",
    "| Gradient Boosting Tree |    0.78   |  0.96  | 0.86 |   0.99   |    24.30    |       -84.57      |       -25.41      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
